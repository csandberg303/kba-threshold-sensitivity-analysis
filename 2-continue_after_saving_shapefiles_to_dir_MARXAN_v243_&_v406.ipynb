{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa221e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import io\n",
    "import pathlib\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# from qgis.core import *\n",
    "# import qmarxan_utils as qmu # import runMarxanOnce\n",
    "# import marxanconpy as mx\n",
    "\n",
    "import contextily as cx\n",
    "import earthpy as et\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.plot import plotting_extent\n",
    "import rioxarray as rxr\n",
    "import subprocess\n",
    "\n",
    "import kba_thresh_sa_scripts as ks\n",
    "\n",
    "# set global cache override variable\n",
    "CACHE_OVERRIDE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6fa16",
   "metadata": {},
   "source": [
    "#### Check for 'earth-analytics/data/kba_thresh_sa' directory\n",
    "* If it exists, it will be set as the working directory.\n",
    "* If it doesn't exist, user is prompted to return to first notebook in workflow.\n",
    "\n",
    "#### *IN THE FUTURE -* \n",
    "* Should there also be a check to verify that the 'hex_shp' dir exists, and/or \n",
    "that the 'hex_shp' directory actually contains shapefiles? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1dd032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is set to earth-analytics/data/kba_thresh_sa.\n"
     ]
    }
   ],
   "source": [
    "# Define a filepath to 'earth-analytics/data/kba_thresh_sa' directory\n",
    "data_path = os.path.normpath(os.path.join(et.io.HOME, \n",
    "                                          'earth-analytics', \n",
    "                                          'data', \n",
    "                                          'kba_thresh_sa'))\n",
    "\n",
    "# Check if 'kba_thresh_sa' directory exists.  If it doesn't, prompt user to \n",
    "# return to the first notebook to begin workflow.  If it does, change working\n",
    "# directory to 'earth-analytics/data/kba_thresh_sa', and define the path to \n",
    "# hex files directory that was created in the first notebook.\n",
    "if os.path.exists(data_path):\n",
    " print('Working directory is set to earth-analytics/data/kba_thresh_sa.')\n",
    " os.chdir(data_path)\n",
    " # define the path to the hexfiles that was created in the 1st notebook\n",
    " shp_data_path = os.path.normpath(os.path.join(data_path, 'hex_shp'))\n",
    "else:\n",
    " print(\"Please go to first notebook in workflow to set up initial'\\\n",
    "      'directories\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9e6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to Marxan.exe executable file has been manually copied over to \n",
    "# 'kba_thresh_sa' directory (maybe it can be copied to there from repo?\n",
    "\n",
    "# v4.0.6 (might be causing 'target2' crash? use 2.43 instead)\n",
    "marxan_path = os.path.join(data_path, \"Marxan_x64.exe\")\n",
    "\n",
    "# v2.43\n",
    "marxan_243_path = os.path.join(data_path, 'Marxan_x64_243.exe')\n",
    "\n",
    "# v1.8.10\n",
    "marxan_1810_path = os.path.join(data_path, 'Marxan_1_8_10.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976ca3c",
   "metadata": {},
   "source": [
    "#### Save table of information to 'earth-analytics/data/kba_thresh_sa'  \n",
    "\n",
    "* The workflow requires an associated table, with information about the \n",
    "ecosystems to be analyzed *(Need to provide more detail about what specific \n",
    "information this table requires... Or will we simply work with the full Landfire\n",
    "readme info, which is saved to the repo?  If so, do we need to add a unique \n",
    "one-word 'Short_Name' to each ecosystem listed, or change the file-naming system \n",
    "to use one of the existing numerical unique identifiers - like 'OID' or \n",
    "'Value'?)*.  \n",
    "\n",
    "* This file will be saved locally to the 'earth-analytics/data/kba_thresh_sa' \n",
    "directory.  \n",
    "\n",
    "* In our inital workflow, we are using the 'LF_EVT_2020_README' file that was\n",
    "provided along with the Landfire raster.  \n",
    "&nbsp; I've manually edited this file to  \n",
    "&emsp; 1. show only the rows for the nine ecosystems selected for initial \n",
    "analysis.  \n",
    "&emsp; 2. add a new column to show the one word short name Lana used when creating \n",
    "her initial files in ArcGIS.  \n",
    "\n",
    "* This file has been manually uploaded to our GitHub repo as 'Assets/Data/\n",
    "from_LF_EVT_2020_README.csv'. \n",
    "\n",
    "* The code below will download that file from URL to a pandas dataframe, then\n",
    "save that dataframe locally as a csv.  \n",
    "\n",
    "#### *IN THE FUTURE -* \n",
    "* This existing code could be reused if the user were prompted for a url where \n",
    "they have their table stored?\n",
    "* Or,  \n",
    "&emsp; 1. prompt user to save their table to 'earth-analytics/data/kba_thresh_sa' \n",
    "as specifically named 'ecosystem_info.csv'  \n",
    "&emsp; 2. Then check for 'ecosystem_info.csv' in \n",
    "'earth-analytics/data/kba_thresh_sa'  \n",
    "&emsp; 3. If found, load to dataframe  \n",
    "&emsp; &emsp; If not found, prompt user to \"Save ecosystm_info.csv' to 'earth-analytics\n",
    "/data/kba_thresh_sa' directory, then rerun notebook\" \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd802b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID</th>\n",
       "      <th>Value</th>\n",
       "      <th>Count_30m</th>\n",
       "      <th>US_hectare</th>\n",
       "      <th>US_km2</th>\n",
       "      <th>EVT_Name_1</th>\n",
       "      <th>LFRDB</th>\n",
       "      <th>elcode</th>\n",
       "      <th>element_gl</th>\n",
       "      <th>NatureServ</th>\n",
       "      <th>...</th>\n",
       "      <th>A3_FINAL</th>\n",
       "      <th>B1_FINAL</th>\n",
       "      <th>B2_FINAL</th>\n",
       "      <th>C3_FINAL</th>\n",
       "      <th>D3_FINAL</th>\n",
       "      <th>RLE_FINAL</th>\n",
       "      <th>GRANK_EQUI</th>\n",
       "      <th>RED</th>\n",
       "      <th>GREEN</th>\n",
       "      <th>BLUE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Short_Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prairie</th>\n",
       "      <td>132</td>\n",
       "      <td>7142</td>\n",
       "      <td>106116</td>\n",
       "      <td>9550</td>\n",
       "      <td>96</td>\n",
       "      <td>Columbia Basin Palouse Prairie</td>\n",
       "      <td>7142</td>\n",
       "      <td>CES304.792</td>\n",
       "      <td>722880</td>\n",
       "      <td>Columbia Basin Palouse Prairie</td>\n",
       "      <td>...</td>\n",
       "      <td>CR</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>EN</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>204</td>\n",
       "      <td>252</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foothill</th>\n",
       "      <td>137</td>\n",
       "      <td>7147</td>\n",
       "      <td>4546277</td>\n",
       "      <td>409165</td>\n",
       "      <td>4092</td>\n",
       "      <td>Western Great Plains Foothill and Piedmont Gra...</td>\n",
       "      <td>7147</td>\n",
       "      <td>CES303.817</td>\n",
       "      <td>722856</td>\n",
       "      <td>Western Great Plains Foothill and Piedmont Gra...</td>\n",
       "      <td>...</td>\n",
       "      <td>VU</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>NT</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>G3</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mesic</th>\n",
       "      <td>224</td>\n",
       "      <td>7322</td>\n",
       "      <td>1090956</td>\n",
       "      <td>98186</td>\n",
       "      <td>982</td>\n",
       "      <td>Crowley's Ridge Mesic Loess Slope Forest</td>\n",
       "      <td>7322</td>\n",
       "      <td>CES203.079</td>\n",
       "      <td>798100</td>\n",
       "      <td>Crowley's Ridge Mesic Loess Slope Forest</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR (EN-CR)</td>\n",
       "      <td>G1G2</td>\n",
       "      <td>144</td>\n",
       "      <td>201</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bluff</th>\n",
       "      <td>229</td>\n",
       "      <td>7327</td>\n",
       "      <td>2050154</td>\n",
       "      <td>184514</td>\n",
       "      <td>1845</td>\n",
       "      <td>East Gulf Coastal Plain Northern Loess Bluff F...</td>\n",
       "      <td>7327</td>\n",
       "      <td>CES203.481</td>\n",
       "      <td>723105</td>\n",
       "      <td>East Gulf Coastal Plain Northern Loess Bluff F...</td>\n",
       "      <td>...</td>\n",
       "      <td>EN</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU-EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>G2</td>\n",
       "      <td>149</td>\n",
       "      <td>143</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pine</th>\n",
       "      <td>244</td>\n",
       "      <td>7346</td>\n",
       "      <td>5015841</td>\n",
       "      <td>451426</td>\n",
       "      <td>4514</td>\n",
       "      <td>Atlantic Coastal Plain Fall-line Sandhills Lon...</td>\n",
       "      <td>7346</td>\n",
       "      <td>CES203.254</td>\n",
       "      <td>723231</td>\n",
       "      <td>Atlantic Coastal Plain Fall-line Sandhills Lon...</td>\n",
       "      <td>...</td>\n",
       "      <td>EN</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>EN (EN-CR)</td>\n",
       "      <td>G1G2</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tallgrass</th>\n",
       "      <td>314</td>\n",
       "      <td>7421</td>\n",
       "      <td>10225903</td>\n",
       "      <td>920331</td>\n",
       "      <td>9203</td>\n",
       "      <td>Central Tallgrass Prairie</td>\n",
       "      <td>7421</td>\n",
       "      <td>CES205.683</td>\n",
       "      <td>722976</td>\n",
       "      <td>Central Tallgrass Prairie</td>\n",
       "      <td>...</td>\n",
       "      <td>CR</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>243</td>\n",
       "      <td>201</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dune</th>\n",
       "      <td>323</td>\n",
       "      <td>7431</td>\n",
       "      <td>19717</td>\n",
       "      <td>1775</td>\n",
       "      <td>18</td>\n",
       "      <td>Southwest Florida Dune and Coastal Grassland</td>\n",
       "      <td>7431</td>\n",
       "      <td>CES203.539</td>\n",
       "      <td>723063</td>\n",
       "      <td>Southwest Florida Dune and Coastal Grassland</td>\n",
       "      <td>...</td>\n",
       "      <td>DD</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>NE</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>245</td>\n",
       "      <td>252</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dome</th>\n",
       "      <td>335</td>\n",
       "      <td>7447</td>\n",
       "      <td>900234</td>\n",
       "      <td>81021</td>\n",
       "      <td>810</td>\n",
       "      <td>South Florida Cypress Dome</td>\n",
       "      <td>7447</td>\n",
       "      <td>CES411.365</td>\n",
       "      <td>723151</td>\n",
       "      <td>South Florida Cypress Dome</td>\n",
       "      <td>...</td>\n",
       "      <td>DD</td>\n",
       "      <td>VU</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>G3</td>\n",
       "      <td>54</td>\n",
       "      <td>163</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marsh</th>\n",
       "      <td>676</td>\n",
       "      <td>9197</td>\n",
       "      <td>1634510</td>\n",
       "      <td>147106</td>\n",
       "      <td>1471</td>\n",
       "      <td>Northern Atlantic Coastal Plain Tidal Salt Marsh</td>\n",
       "      <td>9197</td>\n",
       "      <td>CES203.519</td>\n",
       "      <td>723073</td>\n",
       "      <td>Northern Atlantic Coastal Plain Tidal Salt Marsh</td>\n",
       "      <td>...</td>\n",
       "      <td>EN</td>\n",
       "      <td>LC</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>G2</td>\n",
       "      <td>131</td>\n",
       "      <td>173</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            OID  Value  Count_30m  US_hectare  US_km2  \\\n",
       "Short_Name                                              \n",
       "prairie     132   7142     106116        9550      96   \n",
       "foothill    137   7147    4546277      409165    4092   \n",
       "mesic       224   7322    1090956       98186     982   \n",
       "bluff       229   7327    2050154      184514    1845   \n",
       "pine        244   7346    5015841      451426    4514   \n",
       "tallgrass   314   7421   10225903      920331    9203   \n",
       "dune        323   7431      19717        1775      18   \n",
       "dome        335   7447     900234       81021     810   \n",
       "marsh       676   9197    1634510      147106    1471   \n",
       "\n",
       "                                                   EVT_Name_1  LFRDB  \\\n",
       "Short_Name                                                             \n",
       "prairie                        Columbia Basin Palouse Prairie   7142   \n",
       "foothill    Western Great Plains Foothill and Piedmont Gra...   7147   \n",
       "mesic                Crowley's Ridge Mesic Loess Slope Forest   7322   \n",
       "bluff       East Gulf Coastal Plain Northern Loess Bluff F...   7327   \n",
       "pine        Atlantic Coastal Plain Fall-line Sandhills Lon...   7346   \n",
       "tallgrass                           Central Tallgrass Prairie   7421   \n",
       "dune             Southwest Florida Dune and Coastal Grassland   7431   \n",
       "dome                               South Florida Cypress Dome   7447   \n",
       "marsh        Northern Atlantic Coastal Plain Tidal Salt Marsh   9197   \n",
       "\n",
       "                elcode  element_gl  \\\n",
       "Short_Name                           \n",
       "prairie     CES304.792      722880   \n",
       "foothill    CES303.817      722856   \n",
       "mesic       CES203.079      798100   \n",
       "bluff       CES203.481      723105   \n",
       "pine        CES203.254      723231   \n",
       "tallgrass   CES205.683      722976   \n",
       "dune        CES203.539      723063   \n",
       "dome        CES411.365      723151   \n",
       "marsh       CES203.519      723073   \n",
       "\n",
       "                                                   NatureServ  ... A3_FINAL  \\\n",
       "Short_Name                                                     ...            \n",
       "prairie                        Columbia Basin Palouse Prairie  ...       CR   \n",
       "foothill    Western Great Plains Foothill and Piedmont Gra...  ...       VU   \n",
       "mesic                Crowley's Ridge Mesic Loess Slope Forest  ...       LC   \n",
       "bluff       East Gulf Coastal Plain Northern Loess Bluff F...  ...       EN   \n",
       "pine        Atlantic Coastal Plain Fall-line Sandhills Lon...  ...       EN   \n",
       "tallgrass                           Central Tallgrass Prairie  ...       CR   \n",
       "dune             Southwest Florida Dune and Coastal Grassland  ...       DD   \n",
       "dome                               South Florida Cypress Dome  ...       DD   \n",
       "marsh        Northern Atlantic Coastal Plain Tidal Salt Marsh  ...       EN   \n",
       "\n",
       "           B1_FINAL B2_FINAL  C3_FINAL D3_FINAL   RLE_FINAL GRANK_EQUI  RED  \\\n",
       "Short_Name                                                                    \n",
       "prairie          LC       LC        EN       VU          CR         G1  204   \n",
       "foothill         LC       LC        NT       LC          VU         G3  218   \n",
       "mesic            EN       EN        VU       CR  CR (EN-CR)       G1G2  144   \n",
       "bluff            LC       LC     VU-EN       EN          EN         G2  149   \n",
       "pine             LC       LC        VU       CR  EN (EN-CR)       G1G2   70   \n",
       "tallgrass        LC       LC        DD       CR          CR         G1  243   \n",
       "dune             LC       LC        NE       CR          CR         G1  245   \n",
       "dome             VU       LC        DD       LC          VU         G3   54   \n",
       "marsh            LC       LC        DD       EN          EN         G2  131   \n",
       "\n",
       "           GREEN BLUE  \n",
       "Short_Name             \n",
       "prairie      252  105  \n",
       "foothill     238  243  \n",
       "mesic        201  143  \n",
       "bluff        143   26  \n",
       "pine          96   32  \n",
       "tallgrass    201   28  \n",
       "dune         252  179  \n",
       "dome         163  120  \n",
       "marsh        173  223  \n",
       "\n",
       "[9 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the csv file stored on GitHub repository \n",
    "# (contains info on selected ecosystems taken from LF_EVT_2020_README \n",
    "# file, with an added 'Short_Name' field that is used as index)\n",
    "\n",
    "# Provide the URL (using raw content at GitHub)\n",
    "ecoinfo_url = (\"https://raw.githubusercontent.com/csandberg303/\"\n",
    "               \"kba-threshold-sensitivity-analysis/main/assets/data/\"\n",
    "               \"from_LF_EVT_2020_README.csv\")\n",
    "\n",
    "# Create local cache overide variable\n",
    "cache_override = True or CACHE_OVERRIDE\n",
    "\n",
    "# Provide the path to local directory\n",
    "ecoinfo_path = os.path.normpath(\n",
    "    os.path.join(data_path, 'from_LF_EVT_2020_README.csv'))\n",
    "\n",
    "# Create dataframe from information at provided URL\n",
    "ecoinfo_df = pd.read_csv(ecoinfo_url).set_index('Short_Name')\n",
    "\n",
    "# Check for csv in local directory and create from df if needed\n",
    "if not os.path.exists(ecoinfo_path) or cache_override:\n",
    "    # Read csv at URL into pandas dataframe, using 'Short_Name' col as index\n",
    "    ecoinfo_df.to_csv(ecoinfo_path)\n",
    "    \n",
    "ecoinfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d67038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OID</th>\n",
       "      <th>Value</th>\n",
       "      <th>Count_30m</th>\n",
       "      <th>US_hectare</th>\n",
       "      <th>US_km2</th>\n",
       "      <th>EVT_Name_1</th>\n",
       "      <th>LFRDB</th>\n",
       "      <th>elcode</th>\n",
       "      <th>element_gl</th>\n",
       "      <th>NatureServ</th>\n",
       "      <th>...</th>\n",
       "      <th>B2_FINAL</th>\n",
       "      <th>C3_FINAL</th>\n",
       "      <th>D3_FINAL</th>\n",
       "      <th>RLE_FINAL</th>\n",
       "      <th>GRANK_EQUI</th>\n",
       "      <th>RED</th>\n",
       "      <th>GREEN</th>\n",
       "      <th>BLUE</th>\n",
       "      <th>Type</th>\n",
       "      <th>Current_IUCN_TH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Short_Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prairie</th>\n",
       "      <td>132</td>\n",
       "      <td>7142</td>\n",
       "      <td>106116</td>\n",
       "      <td>9550</td>\n",
       "      <td>96</td>\n",
       "      <td>Columbia Basin Palouse Prairie</td>\n",
       "      <td>7142</td>\n",
       "      <td>CES304.792</td>\n",
       "      <td>722880</td>\n",
       "      <td>Columbia Basin Palouse Prairie</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>EN</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>204</td>\n",
       "      <td>252</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foothill</th>\n",
       "      <td>137</td>\n",
       "      <td>7147</td>\n",
       "      <td>4546277</td>\n",
       "      <td>409165</td>\n",
       "      <td>4092</td>\n",
       "      <td>Western Great Plains Foothill and Piedmont Gra...</td>\n",
       "      <td>7147</td>\n",
       "      <td>CES303.817</td>\n",
       "      <td>722856</td>\n",
       "      <td>Western Great Plains Foothill and Piedmont Gra...</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>NT</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>G3</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mesic</th>\n",
       "      <td>224</td>\n",
       "      <td>7322</td>\n",
       "      <td>1090956</td>\n",
       "      <td>98186</td>\n",
       "      <td>982</td>\n",
       "      <td>Crowley's Ridge Mesic Loess Slope Forest</td>\n",
       "      <td>7322</td>\n",
       "      <td>CES203.079</td>\n",
       "      <td>798100</td>\n",
       "      <td>Crowley's Ridge Mesic Loess Slope Forest</td>\n",
       "      <td>...</td>\n",
       "      <td>EN</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR (EN-CR)</td>\n",
       "      <td>G1G2</td>\n",
       "      <td>144</td>\n",
       "      <td>201</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bluff</th>\n",
       "      <td>229</td>\n",
       "      <td>7327</td>\n",
       "      <td>2050154</td>\n",
       "      <td>184514</td>\n",
       "      <td>1845</td>\n",
       "      <td>East Gulf Coastal Plain Northern Loess Bluff F...</td>\n",
       "      <td>7327</td>\n",
       "      <td>CES203.481</td>\n",
       "      <td>723105</td>\n",
       "      <td>East Gulf Coastal Plain Northern Loess Bluff F...</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU-EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>G2</td>\n",
       "      <td>149</td>\n",
       "      <td>143</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pine</th>\n",
       "      <td>244</td>\n",
       "      <td>7346</td>\n",
       "      <td>5015841</td>\n",
       "      <td>451426</td>\n",
       "      <td>4514</td>\n",
       "      <td>Atlantic Coastal Plain Fall-line Sandhills Lon...</td>\n",
       "      <td>7346</td>\n",
       "      <td>CES203.254</td>\n",
       "      <td>723231</td>\n",
       "      <td>Atlantic Coastal Plain Fall-line Sandhills Lon...</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>CR</td>\n",
       "      <td>EN (EN-CR)</td>\n",
       "      <td>G1G2</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tallgrass</th>\n",
       "      <td>314</td>\n",
       "      <td>7421</td>\n",
       "      <td>10225903</td>\n",
       "      <td>920331</td>\n",
       "      <td>9203</td>\n",
       "      <td>Central Tallgrass Prairie</td>\n",
       "      <td>7421</td>\n",
       "      <td>CES205.683</td>\n",
       "      <td>722976</td>\n",
       "      <td>Central Tallgrass Prairie</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>243</td>\n",
       "      <td>201</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dune</th>\n",
       "      <td>323</td>\n",
       "      <td>7431</td>\n",
       "      <td>19717</td>\n",
       "      <td>1775</td>\n",
       "      <td>18</td>\n",
       "      <td>Southwest Florida Dune and Coastal Grassland</td>\n",
       "      <td>7431</td>\n",
       "      <td>CES203.539</td>\n",
       "      <td>723063</td>\n",
       "      <td>Southwest Florida Dune and Coastal Grassland</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>NE</td>\n",
       "      <td>CR</td>\n",
       "      <td>CR</td>\n",
       "      <td>G1</td>\n",
       "      <td>245</td>\n",
       "      <td>252</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dome</th>\n",
       "      <td>335</td>\n",
       "      <td>7447</td>\n",
       "      <td>900234</td>\n",
       "      <td>81021</td>\n",
       "      <td>810</td>\n",
       "      <td>South Florida Cypress Dome</td>\n",
       "      <td>7447</td>\n",
       "      <td>CES411.365</td>\n",
       "      <td>723151</td>\n",
       "      <td>South Florida Cypress Dome</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>LC</td>\n",
       "      <td>VU</td>\n",
       "      <td>G3</td>\n",
       "      <td>54</td>\n",
       "      <td>163</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marsh</th>\n",
       "      <td>676</td>\n",
       "      <td>9197</td>\n",
       "      <td>1634510</td>\n",
       "      <td>147106</td>\n",
       "      <td>1471</td>\n",
       "      <td>Northern Atlantic Coastal Plain Tidal Salt Marsh</td>\n",
       "      <td>9197</td>\n",
       "      <td>CES203.519</td>\n",
       "      <td>723073</td>\n",
       "      <td>Northern Atlantic Coastal Plain Tidal Salt Marsh</td>\n",
       "      <td>...</td>\n",
       "      <td>LC</td>\n",
       "      <td>DD</td>\n",
       "      <td>EN</td>\n",
       "      <td>EN</td>\n",
       "      <td>G2</td>\n",
       "      <td>131</td>\n",
       "      <td>173</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            OID  Value  Count_30m  US_hectare  US_km2  \\\n",
       "Short_Name                                              \n",
       "prairie     132   7142     106116        9550      96   \n",
       "foothill    137   7147    4546277      409165    4092   \n",
       "mesic       224   7322    1090956       98186     982   \n",
       "bluff       229   7327    2050154      184514    1845   \n",
       "pine        244   7346    5015841      451426    4514   \n",
       "tallgrass   314   7421   10225903      920331    9203   \n",
       "dune        323   7431      19717        1775      18   \n",
       "dome        335   7447     900234       81021     810   \n",
       "marsh       676   9197    1634510      147106    1471   \n",
       "\n",
       "                                                   EVT_Name_1  LFRDB  \\\n",
       "Short_Name                                                             \n",
       "prairie                        Columbia Basin Palouse Prairie   7142   \n",
       "foothill    Western Great Plains Foothill and Piedmont Gra...   7147   \n",
       "mesic                Crowley's Ridge Mesic Loess Slope Forest   7322   \n",
       "bluff       East Gulf Coastal Plain Northern Loess Bluff F...   7327   \n",
       "pine        Atlantic Coastal Plain Fall-line Sandhills Lon...   7346   \n",
       "tallgrass                           Central Tallgrass Prairie   7421   \n",
       "dune             Southwest Florida Dune and Coastal Grassland   7431   \n",
       "dome                               South Florida Cypress Dome   7447   \n",
       "marsh        Northern Atlantic Coastal Plain Tidal Salt Marsh   9197   \n",
       "\n",
       "                elcode  element_gl  \\\n",
       "Short_Name                           \n",
       "prairie     CES304.792      722880   \n",
       "foothill    CES303.817      722856   \n",
       "mesic       CES203.079      798100   \n",
       "bluff       CES203.481      723105   \n",
       "pine        CES203.254      723231   \n",
       "tallgrass   CES205.683      722976   \n",
       "dune        CES203.539      723063   \n",
       "dome        CES411.365      723151   \n",
       "marsh       CES203.519      723073   \n",
       "\n",
       "                                                   NatureServ  ... B2_FINAL  \\\n",
       "Short_Name                                                     ...            \n",
       "prairie                        Columbia Basin Palouse Prairie  ...       LC   \n",
       "foothill    Western Great Plains Foothill and Piedmont Gra...  ...       LC   \n",
       "mesic                Crowley's Ridge Mesic Loess Slope Forest  ...       EN   \n",
       "bluff       East Gulf Coastal Plain Northern Loess Bluff F...  ...       LC   \n",
       "pine        Atlantic Coastal Plain Fall-line Sandhills Lon...  ...       LC   \n",
       "tallgrass                           Central Tallgrass Prairie  ...       LC   \n",
       "dune             Southwest Florida Dune and Coastal Grassland  ...       LC   \n",
       "dome                               South Florida Cypress Dome  ...       LC   \n",
       "marsh        Northern Atlantic Coastal Plain Tidal Salt Marsh  ...       LC   \n",
       "\n",
       "           C3_FINAL D3_FINAL   RLE_FINAL GRANK_EQUI  RED GREEN BLUE Type  \\\n",
       "Short_Name                                                                 \n",
       "prairie          EN       VU          CR         G1  204   252  105    1   \n",
       "foothill         NT       LC          VU         G3  218   238  243    2   \n",
       "mesic            VU       CR  CR (EN-CR)       G1G2  144   201  143    1   \n",
       "bluff         VU-EN       EN          EN         G2  149   143   26    1   \n",
       "pine             VU       CR  EN (EN-CR)       G1G2   70    96   32    1   \n",
       "tallgrass        DD       CR          CR         G1  243   201   28    1   \n",
       "dune             NE       CR          CR         G1  245   252  179    1   \n",
       "dome             DD       LC          VU         G3   54   163  120    2   \n",
       "marsh            DD       EN          EN         G2  131   173  223    1   \n",
       "\n",
       "           Current_IUCN_TH  \n",
       "Short_Name                  \n",
       "prairie               0.05  \n",
       "foothill              0.10  \n",
       "mesic                 0.05  \n",
       "bluff                 0.05  \n",
       "pine                  0.05  \n",
       "tallgrass             0.05  \n",
       "dune                  0.05  \n",
       "dome                  0.10  \n",
       "marsh                 0.05  \n",
       "\n",
       "[9 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 2 columns to 'ecoinfo_df'\n",
    "\n",
    "# 1st column - Add column 'Type' (needed for CLUZ addin input file \n",
    "#'targets.csv'; might not be needed for marxanconpy) Uses np.select to assign \n",
    "# a number (1 or 2), based upon the string seen in the 'RLE_FINAL' column\n",
    "# (Type = 1 if 'CR', 'CR (CR-EN)', 'EN (CR-EN) or 'EN'; Type = 2 if 'VU')\n",
    "\n",
    "# create a list of conditions\n",
    "type_conditions = [(ecoinfo_df['RLE_FINAL'] == 'CR'), \n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'CR (EN-CR)'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'EN'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'EN (EN-CR)'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'VU')]\n",
    "\n",
    "# create a list of the values to assign for each condition\n",
    "type_values = [1, 1, 1, 1, 2]\n",
    "\n",
    "# create new column using np.select to assign values using lists as arguments\n",
    "ecoinfo_df['Type'] = np.select(type_conditions, type_values)\n",
    "\n",
    "# 2nd column - Add column 'Current_IUCN_TH'. Uses np.select to assign a \n",
    "# threshold percentage, based upon the column 'Type' (5% if 1, 10% if 2)\n",
    "\n",
    "# create a list of conditions\n",
    "current_threshold_conditions = [(ecoinfo_df['Type'] == 1), \n",
    "                               (ecoinfo_df['Type'] == 2)]\n",
    "\n",
    "# create a list of the values to assign for each condition\n",
    "current_threshold_values = [.05, .10]\n",
    "\n",
    "# create new column using np.select to assign values using lists as arguments\n",
    "ecoinfo_df['Current_IUCN_TH'] = np.select(\n",
    "    current_threshold_conditions, current_threshold_values)\n",
    "\n",
    "ecoinfo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118406c0",
   "metadata": {},
   "source": [
    "#### *IN THE FUTURE -* \n",
    "\n",
    "Currently our code will work with the ecosystem raster and hex files that Lana \n",
    "created in ArcGIS using the ArcMarxan plugin.  Ultimately we hope to work directly\n",
    "with the full Landfire EVT 2020 raster, but the file is proving too large to \n",
    "effectively manage with our personal laptops. A solution may be found using the \n",
    "2016 Landfire data which has an available API (the 2020 data is scheduled to be \n",
    "published to the API later this year). An alternitive solution may be found using \n",
    "Dask, or possibly Amazon Web Services.\n",
    "\n",
    "If/When our code can access the full CONUS raster, the source data in repo assets \n",
    "(and links that file in this code) will need to be updated to the full version of \n",
    "the raster's LF_2020_EVT_README file. Once that occurs, we could ask for user \n",
    "input to get entries matching the 'Values' column in that file, as a way of \n",
    "selecting specific ecosystems from the full Landfire EVT 2020 data. That user \n",
    "input would be assigned to a list variable 'value_filter'. \n",
    "\n",
    "The user would then be prompted for a one-word 'Short_Name' value for each \n",
    "ecosystem being analyzed (ex. mesic, dune, dome), to be used in file naming. This \n",
    "abbreviated name would be added to the ecoinfo_df.  \n",
    "\n",
    "Currently the 'Short_Name' values have been hardcoded, to match what Lana chose \n",
    "when creating her ArcGis files. The 'value_filter' variable will also be \n",
    "hardcoded, to match the values seen in the LF_2020_EVT_README file for the three \n",
    "ecosystems we are using as test data (Crowley's Ridge Mesic Loess Slope Forest, \n",
    "Southwest Florida Dune and Coastal Grassland, and South Florida Cypress Dome).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58116e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dome', 'dune', 'mesic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE LISTS THAT WILL BE USED LATER IN ITERATION LOOPS\n",
    "\n",
    "# Create list of threshold values to test\n",
    "test_threshold = [1.0, 0.75, 0.50, 0.25]\n",
    "\n",
    "# Define list variable 'value_filter' to show the values matching the 'Values' \n",
    "# column of 'ecoinfo_df' for the three ecosystems which have shp and hex files \n",
    "# uploaded to the GitHub repository - 'dome', 'dune', and 'mesic', \n",
    "value_filter = [\n",
    "    7431, # dune\n",
    "    7322, # mesic\n",
    "    7447 # dome\n",
    "    ]\n",
    "\n",
    "# use value_filter to create a new df with only matching records\n",
    "eco_subset_df = ecoinfo_df[ecoinfo_df['Value'].isin(value_filter)]\n",
    "\n",
    "# Create alphabetical list of ecosystems to be analyzed, taken from the \n",
    "# 'Short_Name' column of eco_subset_df\n",
    "eco_list = eco_subset_df.index.values.tolist()\n",
    "eco_list.sort()\n",
    "\n",
    "# print(eco_list)\n",
    "eco_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93362939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES TO BE USED IN MARXAN RUN\n",
    "\n",
    "# provide a testrun_basename (will appear in filename for final summary files)\n",
    "testrun_basename = 'mx1810summarytest'\n",
    "\n",
    "# ESPG value to set as CRS for raster and shapefile\n",
    "espg = '5070'\n",
    "\n",
    "# # Set prop for spec.dat (default value = 30% of total extent) \n",
    "# (must be between 0 and 1, Lana tutorial suggested 0.3)\n",
    "prop = 0.3\n",
    "\n",
    "# Species Penalty Factor - more detail needed... we're using default val of 1\n",
    "spf = 10\n",
    "\n",
    "# Number of repeat runs (or solutions) - orig value in qmarxan = 100\n",
    "numreps = 100\n",
    "\n",
    "# Number of iterations for annealing \n",
    "# orig value 1000000\n",
    "# (RUNMODE 1 & 3 did not complete successfully with numitins=10 (or 1000?)\n",
    "numitns = 10000\n",
    "\n",
    "# Set blm (default value from qmarxan code = 1)\n",
    "# blmtest = [0.1, 0.5, 1, 5, 10] # blmtest used for test loop only, no longer needed\n",
    "blm = 10\n",
    "    \n",
    "# test runmode\n",
    "# runmode_ls = [1 , 3] # runmode_ls used for test loop only, no longer needed\n",
    "runmode = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b275729",
   "metadata": {},
   "source": [
    "#### Loop through the eco_list, create directories and input files needed by Marxan.\n",
    "\n",
    "Each time the code below runs, a new timestamped diretory is created. Inside will\n",
    "be subdirectories created from the 'Short_Name' value of the selected ecosystems \n",
    "seen in the 'eco_subset' variable.\n",
    "\n",
    "Each of these ecosystem subdirectories will have the following named \n",
    "subdirectories -\n",
    "* input - where files needed by marxan analysis are stored (bound.dat, pu.dat, \n",
    "puvsp.dat, spec.dat)\n",
    "* output - where files generated by marxan analysis are stored\n",
    "* pu - pu and report seen in qmarxan setup (purpose tbd)\n",
    "* report - pu and report seen in qmarxan setup (purpose tbd)\n",
    "* source data - where the rasters and PU hex_shp files are moved to, after they\n",
    "are copied from the 'r_tif' and 'hex_shp' folders\n",
    "\n",
    "A fifth input file 'input.dat' is created and placed in the main ecosystem \n",
    "directory.\n",
    "\n",
    "The code below will also create a 'targets.csv' file for each value in \n",
    "the 'test_threshold' list variable.  This is done using the function \n",
    "'create_targets_files'. These 'targets.csv' files are used by the CLUZ \n",
    "plugin in QGIS.  They are not used by the QMarxan QGIS plugin, and ultimately may\n",
    "not be needed for our marxanconpy workflow.  *Perhaps the loop inside\n",
    "the function that uses the 'test_threshold' variable could be reused for another \n",
    "purpose?*\n",
    "\n",
    "Currently, we are using the input files that Lana created using ArcGIS, that have \n",
    "been saved to the repo.  The code below will simply copy those files from the repo \n",
    "and save them into each ecosystem's 'input' folder.\n",
    "\n",
    "#### *IN THE FUTURE -* \n",
    "* Our project sponsor has said that the set of input files are commonly prepared \n",
    "using GIS tools.  If that is the practice we will continue, new files for \n",
    "additional ecosystmes will be generated using QGIS/QMarxan.\n",
    "* Another option may be to create a new function in this workflow for each \n",
    "specific input file (input.dat, bound.dat, pu.dat, puvsp.dat, spec.dat.).  A \n",
    "function has been written to create the input.dat file, using code seen in the \n",
    "qmarxan repository from Apropos Information Systems (used under the GPL-2.0 \n",
    "license). Creating the input files programatically rather than in GIS may allow \n",
    "for easier manipulation of the files within the workflow to perform the \n",
    "sensitivity analysis of the KBA threshold values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee45611",
   "metadata": {},
   "source": [
    " #### Conducting a Sensitivity Analysis of the IUCN TH - spec.dat 'target2'\n",
    " \n",
    " The KBA threshold can be tested using the 'target2' column in the spec.dat \n",
    " input file.  \n",
    "'target2' sets a minimum size for an identified area to count against \n",
    "the target value.  If a patch of selected hexcells does not meet that\n",
    "minimum value, it won't appear in a final solution. \n",
    "This value is calculated for each ecosystem -\n",
    "iterate by - test_threshold = [1.0, 0.75, 0.50, 0.25]\n",
    "'target2' = (total area * 'Current_IUCN_TH') x test_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b601a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS CELL FOR MARXAN v4.06 AND MARXAN v2.43 (FIND CELL BELOW FOR V1.8.10)\n",
    "\n",
    "# RUN THIS CELL TO BEGIN AUTOMATED WORKFLOW (1ST CELL OF TWO - BEGIN MARXAN ANALYSIS)\n",
    "\n",
    "# checks to see if a directory based upon 'testrun_basename' has already been \n",
    "# made, and if so will add a number to the end 'testrun_basename' before \n",
    "# creating new directory.\n",
    "testrun_basename_ck = glob(os.path.join(data_path, '*' + testrun_basename))\n",
    "                              \n",
    "\n",
    "if testrun_basename_ck:\n",
    "    expand = 1\n",
    "    while True:\n",
    "        expand +=1\n",
    "        new_tr_bn = testrun_basename + str(expand)\n",
    "        testrun_basename_ck = glob(os.path.join(data_path, '*' + new_tr_bn))\n",
    "        if testrun_basename_ck:\n",
    "            continue\n",
    "        else:\n",
    "            testrun_basename = new_tr_bn\n",
    "            break\n",
    "            \n",
    "print('testrun_basename: ' + testrun_basename)\n",
    "\n",
    "# set new directory name, based upon timestamp and provided 'testrun_basename'\n",
    "new_dir = os.path.normpath(\n",
    "    os.path.join(data_path, datetime.datetime.now().strftime('%Y%m%d_%H%M%S') \n",
    "                 + '_' + testrun_basename))\n",
    "\n",
    "os.makedirs(new_dir)\n",
    "os.chdir(new_dir)\n",
    "print(new_dir + '\\n')\n",
    "\n",
    "# set heurtype - determined by runmode\n",
    "# if RUNMODE = 3 then use heurtype = 1 (greedy), else -1 (not used)\n",
    "if runmode == 3:\n",
    "    heurtype = 1\n",
    "else:\n",
    "    heurtype = -1\n",
    "print('runmode: ' + str(runmode) +'\\nheurtype: ' + str(heurtype) +'\\n')\n",
    "\n",
    "\n",
    "### 1ST LOOP BEGINS HERE \n",
    "\n",
    "# LOOP THROUGH ECOSYSTEMS (in 'ecolist') FOR EACH TEST (in 'test_threshold')\n",
    "for test in test_threshold:\n",
    "    for eco in eco_list:\n",
    "\n",
    "        # create Scenario ID - used as a prefix in the output files \n",
    "        scen_id = eco + str(test).translate(str.maketrans('', '', '.'))\n",
    "\n",
    "        # Print first info statement, begin creating needed directories\n",
    "        print(\"Begin run: \" + scen_id)\n",
    "\n",
    "        os.makedirs(scen_id) \n",
    "        ecotest_data_path = os.path.normpath(os.path.join(data_path, new_dir, \n",
    "                                                          scen_id))\n",
    "        print(ecotest_data_path)\n",
    "        os.chdir(ecotest_data_path)\n",
    "        os.makedirs('source_data')\n",
    "        os.chdir('source_data')\n",
    "\n",
    "        # set 'target2' variable \n",
    "        # to equal KBA Threshold Value for Ecosystem at Test Level\n",
    "        # target2 = US_m2 x Current_IUCN_TH x threshold test level\n",
    "        current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "        us_km2 = eco_subset_df.at[eco,'US_km2']\n",
    "        us_m2 = us_km2 * 1000000\n",
    "\n",
    "        target2 = test * current_iucn_th * us_m2\n",
    "        target2 = round(target2)\n",
    "\n",
    "        # set the 'target' used in spec.dat file to equal 'target2'\n",
    "        # (for target to equal 30% proportion, use target = us_m2 * 0.3)\n",
    "        target = target2\n",
    "\n",
    "        # print variables in run to screen for reference\n",
    "        print('\\n US-km2: ' + str(us_km2) + \n",
    "              '\\n US_m2: ' + str(us_m2) + \n",
    "              '\\n target: ' + str(target) + \n",
    "              '\\n Current_IUCN_TH: ' + str(current_iucn_th) + \n",
    "              '\\n threshold test level: ' + str(test) +\n",
    "              '\\n\\n target2 = US_m2 x Current_IUCN_TH x threshold test level'\n",
    "              '\\n target2: ' + str(target2) + '\\n\\n'\n",
    "              '\\n BLM: ' + str(blm) + '\\n' +\n",
    "              '\\n SPF: ' + str(spf) + '\\n')\n",
    "\n",
    "        # copy source files that were stored to the 'hex_shp' and 'r_tif' \n",
    "        # directories after running 1st notebook. Our workflow is currently  \n",
    "        # using the files Lana created manually using ArcGIS\n",
    "        ks.get_source_files(os.path.join(data_path, \"hex_shp\"), eco, scen_id)\n",
    "        ks.get_source_files(os.path.join(data_path, \"r_tif\"), eco, scen_id)\n",
    "        os.chdir(ecotest_data_path)\n",
    "\n",
    "        # create 'input.dat' from formula adapted from 'qmarxan_toolbox' code,\n",
    "        # which includes the 'formatAsME' - format as Marxan Exponent format\n",
    "        # function. Some input parameters are provided, to replace the default\n",
    "        # values provided in the qmarxan code. \n",
    "        # (At times the input.dat values have been edited directly in the \n",
    "        # input.dat formula, when a specific setting was needed for workflow (ex. ???).\n",
    "        ks.create_input_dat(ecotest_data_path, \n",
    "                            blm, numreps, \n",
    "                            numitns, \n",
    "                            runmode, \n",
    "                            heurtype, \n",
    "                            scen_id)\n",
    "        input_dat_path = os.path.normpath(os.path.join(\n",
    "            ecotest_data_path, \"input.dat\"))\n",
    "        input_dat = pd.read_csv(input_dat_path)\n",
    "        print(input_dat)\n",
    "\n",
    "        # create input directory, which is where four additional .dat files \n",
    "        # will be stored\n",
    "        os.makedirs('input')\n",
    "        eco_input_data_path = os.path.normpath(os.path.join(\n",
    "            ecotest_data_path, 'input'))\n",
    "        os.chdir(eco_input_data_path)\n",
    "\n",
    "        # create pu.dat from original formula. \n",
    "        # Provides a record of each planning unit hex cell in the .shp file, with \n",
    "        # using a default uniform cost of '1', and  a status of '0' which \n",
    "        # indicates that unit is avaialable to Marxan for selection).\n",
    "        ks.create_pu_dat(eco, \n",
    "                         ecotest_data_path, \n",
    "                         scen_id)  \n",
    "        pu_dat_path = os.path.normpath(os.path.join(\n",
    "            ecotest_data_path, 'input', \"pu.dat\"))\n",
    "        pu_dat = pd.read_csv(pu_dat_path)\n",
    "        pu_dat.info()\n",
    "\n",
    "        # create spec.dat from v1 formula (includes 'prop' only)\n",
    "#         ks.create_spec_dat_v1(eco_subset_df, eco, prop, spf, scen_id)\n",
    "        \n",
    "        # create spec.dat from v2 formula (includes 'prop' and 'target2')\n",
    "        ks.create_spec_dat_v2(eco_subset_df, prop, target2, spf, eco, scen_id)\n",
    "        \n",
    "        # create spec.dat from v3 formula (includes 'target' and 'target2') - use with v1810\n",
    "#         ks.create_spec_dat_v3(eco_subset_df, target_1810, target2, spf, eco, scen_id)\n",
    "        \n",
    "        # create spec.dat from v4 formula (includes 'target', no 'target2')\n",
    "#         ks.create_spec_dat_v4(eco_subset_df, eco, target, spf, scen_id)\n",
    "\n",
    "        spec_dat_path = os.path.normpath(os.path.join(\n",
    "            ecotest_data_path, 'input', \"spec.dat\"))\n",
    "        spec_dat = pd.read_csv(spec_dat_path)\n",
    "        spec_dat.info()\n",
    "\n",
    "        # use 'get_marxan_input_files' function to copy in any remaining .dat \n",
    "        # files needed (until formulas can be written to create these files). \n",
    "        # This formula currently is used for 'bound.dat' and 'puvsp.dat'.\n",
    "        # Formula will copy files that have been created using ArcMarxan tool \n",
    "        # in ArcGIS then saved to the repository.\n",
    "        ks.get_marxan_input_files(eco, \n",
    "                                  [\"bound.dat\", \n",
    "        #                                     \"pu.dat\", \n",
    "                                  \"puvsp.dat\", \n",
    "        #                                     \"spec.dat\"\n",
    "                                  ],\n",
    "                                 scen_id)\n",
    "\n",
    "        # create remaining directories\n",
    "        os.chdir(ecotest_data_path)\n",
    "        os.makedirs('output')\n",
    "        os.makedirs('report')\n",
    "        os.makedirs('pu')\n",
    "\n",
    "        # call on marxan executable to run analysis (currently using v2.43)\n",
    "        os.startfile(marxan_243_path)\n",
    "        \n",
    "        print (scen_id + \": MARXAN ANALYSIS INITIATED\")\n",
    "        os.chdir(new_dir)\n",
    "            \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting this line to create break in notebook execution while marxan runs\n",
    "break_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5736ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USE THIS CELL FOR 2ND PART OF WORKFLOW - MARXAN 4.06 or MARXAN 2.43 ONLY\n",
    "# (USE CELL BELOW FOR 1.8.10)\n",
    "\n",
    "# WAIT FOR MARXAN RUNS TO COMPLETE, THEN RUN THIS CELL TO COLLECT SUMMARY INFO \n",
    "# (monitor marxan run screens; run this cell after 'The End' is seen in all)\n",
    "\n",
    "# Define 'dirs' glob list as dirs ending in testrun_basename\n",
    "dirs = glob(os.path.join(data_path, '*' + testrun_basename))\n",
    "\n",
    "# Create empty lists outside of the for loop to store data\n",
    "summary_info = []\n",
    "best_info = []\n",
    "error_list = []\n",
    "bestimage_list = []\n",
    "ssolnimage_list = []\n",
    "\n",
    "# LOOP #2: LOOP THROUGH DIRECTORIES IN DIRS TO CREATE SUMMARY FILES AND PLOTS\n",
    "for dir in dirs:\n",
    "    # repeat 1st loop sequence that returns variables and screen output \n",
    "    os.chdir(dir)\n",
    "    print(os.getcwd())\n",
    "    for test in test_threshold:\n",
    "        for eco in eco_list:\n",
    "            print(os.getcwd())\n",
    "            # NOT SURE IF THIS STEP IS STILL NEEDED, WITHOUT EXTRA LOOPS FOR RUNMODE/BLM TESTS??\n",
    "            scen_id = eco + str(test).translate(str.maketrans('', '', '.'))\n",
    "            ecotest_data_path = os.path.normpath(os.path.join(data_path, dir, \n",
    "                                                              scen_id))\n",
    "            current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "            us_km2 = eco_subset_df.at[eco,'US_km2']\n",
    "            us_m2 = us_km2 * 1000000\n",
    "\n",
    "            target2 = test * current_iucn_th * us_m2\n",
    "            target2 = round(target2)\n",
    "            \n",
    "\n",
    "            # change to output directory to generate original output info \n",
    "            # Currently 4 items - \n",
    "            #1) mxrunsummary, \n",
    "            #2) plots of 'best_run' and 'ssoln', \n",
    "            #3) combined '_sum.csv' file with add'l columns)\n",
    "            #4) combined 'best_run' .csv file, \n",
    "            os.chdir(os.path.normpath(os.path.join(\n",
    "                ecotest_data_path, 'output')))\n",
    "            print(os.getcwd())\n",
    "\n",
    "            # 1 - create expanded summary of info based on the '*_sen.dat' \n",
    "            # scenario details file Marxan generates\n",
    "            ks.create_mxrun_summary(ecotest_data_path, espg, prop, blm, \n",
    "                                    target2, spf, scen_id, eco, eco_subset_df)\n",
    "            print(os.getcwd())\n",
    "            \n",
    "            \n",
    "            # 2 - get plots from 'best_run' file info and 'ssoln' file, if \n",
    "            # those files are available\n",
    "            print(scen_id + \": Begin plotting\")\n",
    "            # 'get_output_plots' function will set crs of source files, check\n",
    "            # if the 'best_run' and 'ssoln' files are avaialble, and if so the  \n",
    "            # 'best_run' and/or 'ssoln' info will be plotted and saved to an\n",
    "            # image which will be saved to pdf file in the 'new_dir' directory\n",
    "            \n",
    "            #USE THIS FUNCTION FOR MARXAN 243 AND 406\n",
    "            ks.get_output_plots (ecotest_data_path, eco, espg, target2, scen_id, us_m2)       \n",
    "\n",
    "            \n",
    "            # convert 'best_run' plot to image and add to bestimage_list, so \n",
    "            # that it'll be added to final pdf of best plot images\n",
    "            best_im = glob(os.path.normpath(os.path.join(ecotest_data_path, \n",
    "                                                         'output', \n",
    "                                                         '*_best_plot_w_bestrun_over_raster.png')))\n",
    "            best_im = Image.open(best_im[0])\n",
    "            best_im = best_im.convert('RGB')\n",
    "            bestimage_list.append(best_im)\n",
    "            print(scen_id + (\": '_best_plot_w_bestrun_over_raster.png' image added to 'bestimage_list' \" \n",
    "                  \"to be included in 'final best_plot pdf'\"))\n",
    "\n",
    "            # IGNORING 'SSOLN' PLOTS FOR NOW\n",
    "            # convert 'ssoln' plot to image and add to ssolnimage_list, so \n",
    "#             # that it'll be added to final pdf of ssoln plot images\n",
    "#             ssoln_im = glob(os.path.normpath(os.path.join(ecotest_data_path, \n",
    "#                                                          'output', \n",
    "#                                                          '*ssoln_plot.png')))\n",
    "#             ssoln_im = Image.open(ssoln_im[0])\n",
    "#             ssoln_im = ssoln_im.convert('RGB')\n",
    "#             ssolnimage_list.append(ssoln_im)\n",
    "#             print(scen_id + (\": 'ssoln_plot' image added to 'ssolnimage_list'\" \n",
    "#                   \" to be included in 'final ssoln_plot pdf'\"))  \n",
    "                                 \n",
    "#             os.chdir(dir)\n",
    "#             print((\"About to save the summary info, incl pdf plots... \\n\"\n",
    "#                   \"cwd = \" + os.getcwd()))\n",
    "\n",
    "            # 3 - save summary info from '_sum.csv' file (if the file exists)\n",
    "            # include 'amount's from best solution plot \n",
    "            out_sum_path = os.path.normpath(os.path.join(ecotest_data_path, \n",
    "                                                         'output', \n",
    "                                                         scen_id + \n",
    "                                                         '_sum.csv'))\n",
    "            # check if the '_sum.csv' file was successfully created by Marxan:\n",
    "            # IF SO, create df from file and add columns 'scen_id', 'runmode',\n",
    "            # 'blm', 'spf' 'prop/target value' (and target2?  THIS COULD BE REVISED/UPDATED)\n",
    "            if os.path.exists(out_sum_path):\n",
    "                os.getcwd()\n",
    "                out_sum_df = pd.read_csv(out_sum_path)\n",
    "                out_sum_df['scen_id'] = scen_id\n",
    "                out_sum_df['runmode'] = runmode\n",
    "                out_sum_df['blm'] = blm\n",
    "                out_sum_df['spf'] = spf\n",
    "#                 spec_dat_path = os.path.normpath(os.path.join(ecootest_data_path, 'input', \"spec.dat\"))\n",
    "                spec_dat_df = pd.read_csv(spec_dat_path)\n",
    "#                 out_sum_df[\"spec.dat 'target' value\"] = spec_dat.columns[1]\n",
    "                out_sum_df['spec.dat target value'] = spec_dat.iat[0,1] # STILL NOT RIGHT\n",
    "#                 data_from_best_dbf = glob(os.path.normpath(os.path.join(ecotest_data_path, 'output', '*w_best_and_ssoln.dbf')))\n",
    "# #             best_im = glob(os.path.normpath(os.path.join(ecotest_data_path, \n",
    "# #                                                          'output', \n",
    "# #                                                          '*best_plot.png')))\n",
    "#                 best_results = data_from_best_dbf[0]\n",
    "#                 best_results_df = pd.read_csv(best_results)\n",
    "#                 out_sum_df['total extent'] = best_results['amount'].sum()\n",
    "#                 out_sum_df['selected amount'] = [best_results['SOLUTION'] == 1, 'amount'].sum()\n",
    "                current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "                out_sum_df['Current_IUCN_TH'] = current_iucn_th\n",
    "                out_sum_df['KBA m2 @ Current_IUCN_TH'] = (eco_subset_df.at[eco,'US_km2']*1000000)*current_iucn_th               \n",
    "\n",
    "                print(scen_id + (\": summary info will be added to \"\n",
    "                                 \"'final_summary.csv'\\n\"))\n",
    "                # Append the data to 'out_sum_df'\n",
    "                summary_info.append(out_sum_df)\n",
    "            # IF IT DOESN'T EXIST, \n",
    "            # print error message and add 'scen_id' to 'error_list'\n",
    "            else:\n",
    "                print(os.getcwd())\n",
    "                print(scen_id + (': run did not complete with a successful '\n",
    "                                 'run summary; added to error list\\n'))\n",
    "                error_list.append([(scen_id + (\": run completed in error, no \"\n",
    "                                               \"'_sum' file found\"))])\n",
    "\n",
    "            # 3 - save best run info from '_best.csv' file (if file exists)\n",
    "            out_best_path = os.path.normpath(os.path.join(ecotest_data_path, \n",
    "                                                          'output', scen_id + \n",
    "                                                          '_best.csv'))\n",
    "            # check if '_best.csv' file exists:\n",
    "            # IF SO, create df from file and add columns 'scen_id', 'runmode' \n",
    "            # 'blm', 'spf' 'prop/target value' (and target2?)\n",
    "            if os.path.exists(out_best_path):\n",
    "                print(os.getcwd())\n",
    "                out_best_df = pd.read_csv(out_best_path)\n",
    "                out_best_df['scen_id'] = scen_id\n",
    "                out_best_df['runmode'] = runmode\n",
    "                out_best_df['blm'] = blm\n",
    "                out_best_df['spf'] = spf\n",
    "                spec_dat_path\n",
    "                out_best_df['spec.dat prop or target'] = spec_dat.columns[1]\n",
    "#                 out_best_df['prop/target value'] = spec_dat.iat[0,1]\n",
    "                print(scen_id + (\": best run info will be added to \"\n",
    "                                 \"'final_best_runs.csv'\\n\"))\n",
    "                # Append the data to 'out_best_df'\n",
    "                best_info.append(out_best_df)\n",
    "            # IF IT DOESN'T EXIST, \n",
    "            # print error message and add 'scen_id' to 'error_list'\n",
    "            else:\n",
    "                print(os.getcwd())\n",
    "                print(scen_id + (\": run did not complete with a successful \"\n",
    "                                 \"'_best' file; added to error list\\n\"))\n",
    "                error_list.append([(scen_id + (\": run completed in error, no\"\n",
    "                                    \"'_best' file found\"))])\n",
    "                \n",
    "\n",
    "                 \n",
    "    # When ecotest loop completes, check if 'summary_info' is an empty list \n",
    "    # (empty list = False)\n",
    "    # If it isn't an empty list, save summary_info to 'final_summary.csv'\n",
    "    if summary_info:\n",
    "        print(os.getcwd())\n",
    "        # Combine all the dataframes stored in 'summary_info' list during the \n",
    "        # loop into one pandas dataframe\n",
    "        final_summary_df = pd.concat(summary_info, axis=0)\n",
    "        final_summary_df.to_csv(testrun_basename + 'final_summary.csv')\n",
    "        print(\"review summary of non-error runs in 'final_summary.csv in \" + dir)    \n",
    "    # if 'summary_info' is an empty list, print error message on screen and\n",
    "    # add to errorlog\n",
    "    else: \n",
    "        print(os.getcwd())\n",
    "        print(\"entire run completed without a successful run summary; added to error list\\n\")\n",
    "        error_list.append([(\"entire (add detail about which run??) run \"\n",
    "                            \"completed without a '_sum' file for any \"\n",
    "                            \"ecosystem at any test level\")])\n",
    "\n",
    "    # check if 'best_info' is an empty list (empty list = False)\n",
    "    # If it isn't, add best_info to 'final_best_runs.csv' \n",
    "    if best_info:\n",
    "        print(os.getcwd())\n",
    "        # Combine all the dataframes stored in the all_df list during the loop \n",
    "        # into one pandas dataframe\n",
    "        pd.concat(best_info)\n",
    "        final_best_df = pd.concat(best_info, axis=0)\n",
    "        final_best_df.to_csv(testrun_basename + 'final_best_runs.csv')\n",
    "        print(\"combined 'best run' info can be found in 'final_best_runs.csv in \" + dir)\n",
    "    # If summary_info is an empty list, print error message on screen and add \n",
    "    # to errorlog\n",
    "    else: \n",
    "        print(os.getcwd())\n",
    "        print(\"entire run did not complete with a successful 'best_run'; added to error list\\n\")\n",
    "        error_list.append([(\"entire (add detail about which run??) run \"\n",
    "                            \"completed without a 'best_run' file for any \"\n",
    "                            \"ecosystem at any test level\")])\n",
    "\n",
    "    # save error_list to 'final_errorlog.csv'\n",
    "    error_list_df = pd.DataFrame(error_list)\n",
    "    error_list_df.to_csv(testrun_basename + 'final_errorlog.csv')\n",
    "    print(\"\\n'final_errorlog.csv' saved to \" + os.getcwd())\n",
    "              \n",
    "    # save plot images to pdf\n",
    "    bestimage_pdf_path = os.path.normpath(os.path.join(\n",
    "        testrun_basename + '_combined_best_plots.pdf'))\n",
    "    bestimage_list[0].save(\n",
    "        bestimage_pdf_path, save_all=True, append_images=bestimage_list)\n",
    "    print(\"\\n'_combined_best_plots.pdf' saved to \" + os.getcwd())\n",
    "    \n",
    "    # ignoring ssoln images for now\n",
    "#     ssolnimage_pdf_path = os.path.normpath(os.path.join(\n",
    "#         testrun_basename + '_combined_ssoln_plots.pdf'))\n",
    "#     ssolnimage_list[0].save(\n",
    "#         ssolnimage_pdf_path, save_all=True, append_images=ssolnimage_list)\n",
    "#     print(\"\\n'_combined_ssoln_plots.pdf' saved to \" + os.getcwd())\n",
    "              \n",
    "print('\\nSUMMARY ANALYSIS RUN COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529e96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST CELL\n",
    "# open 'best' from marxan1810 output and merge with puvsp to get total amounts per selected hexcell\n",
    "# (best from marxan1810 shows single column on selected cells only, ~13 cells out of 500 in dome ex.)\n",
    "\n",
    "ecotest_data_path = os.path.normpath(os.path.join(data_path, '20220621_224409_mx1810summarytest222', 'dome10'))\n",
    "\n",
    "best_marxan_1810_path = glob(os.path.normpath(os.path.join(ecotest_data_path, 'output', '*_best.dat')))\n",
    "best_marxan_1810_path[0]\n",
    "\n",
    "if os.stat(best_marxan_1810_path[0]).st_size == 0 or best_marxan_1810_path == []:\n",
    "    print('143')\n",
    "# best_marxan_1810 = pd.read_csv(best_marxan_1810_path[0], header=None, index_col=False)\n",
    "# best_marxan_1810.columns = ['pu']\n",
    "\n",
    "# puvsp_path = os.path.normpath(os.path.join(ecotest_data_path, 'input', 'puvsp.dat'))\n",
    "# puvsp_dat = pd.read_csv(puvsp_path)\n",
    "# puvsp_dat\n",
    "\n",
    "# best_mx1810_w_amt = pd.merge(puvsp_dat, best_marxan_1810, on ='pu', how ='inner')\n",
    "# best_mx1810_w_amt\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = os.path.getsize(\"best_marxan_1810_path[0]\") == 0\n",
    "test\n",
    "best_marxan_1810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL - try to merge info from .dbf/gpd with pandas dataframe 'out_sum_df' for expanded final summary\n",
    "\n",
    "# data_from_best_dbf = glob(os.path.normpath(os.path.join(ecotest_data_path, 'output', '*w_best_and_ssoln.dbf')))\n",
    "#             best_im = glob(os.path.normpath(os.path.join(ecotest_data_path, \n",
    "#                                                          'output', \n",
    "#                                                          '*best_plot.png')))\n",
    "data_from_best_dbf = os.path.normpath(os.path.join(ecotest_data_path, 'output', eco + '_w_best_and_ssoln.dbf'))\n",
    "\n",
    "\n",
    "\n",
    "# best_results = data_from_best_dbf[0]\n",
    "\n",
    "os.getcwd()\n",
    "out_sum_df = pd.read_csv(out_sum_path)\n",
    "out_sum_df['scen_id'] = scen_id\n",
    "out_sum_df['runmode'] = runmode\n",
    "out_sum_df['blm'] = blm\n",
    "out_sum_df['spf'] = spf\n",
    "#                 spec_dat_path = os.path.normpath(os.path.join(ecootest_data_path, 'input', \"spec.dat\"))\n",
    "spec_dat_df = pd.read_csv(spec_dat_path)\n",
    "#                 out_sum_df[\"spec.dat 'target' value\"] = spec_dat.columns[1]\n",
    "out_sum_df['spec.dat target value'] = spec_dat.iat[0,1]\n",
    "glob = glob(os.path.normpath(os.path.join(ecotest_data_path, 'output', '*w_best_and_ssoln.dbf')))\n",
    "best_results = glob[0]\n",
    "best_results_df = pd.read_csv(best_results)\n",
    "out_sum_df['total extent'] = best_results['amount'].sum()\n",
    "out_sum_df['selected amount'] = [best_results['SOLUTION'] == 1, 'amount'].sum()\n",
    "current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "out_sum_df['Current_IUCN_TH'] = current_iucn_th\n",
    "out_sum_df['KBA m2 @ Current_IUCN_TH'] = (eco_subset_df.at[eco,'US_km2']*1000000)*current_iucn_th   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "path_object = pathlib.path(data_from_best_dbf)\n",
    "df = geopandas.read_file(path_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "out_sum_df = pd.read_csv(out_sum_path)\n",
    "out_sum_df['scen_id'] = scen_id\n",
    "out_sum_df['runmode'] = runmode\n",
    "out_sum_df['blm'] = blm\n",
    "out_sum_df['spf'] = spf\n",
    "#                 spec_dat_path = os.path.normpath(os.path.join(ecootest_data_path, 'input', \"spec.dat\"))\n",
    "spec_dat_df = pd.read_csv(spec_dat_path)\n",
    "#                 out_sum_df[\"spec.dat 'target' value\"] = spec_dat.columns[1]\n",
    "out_sum_df['spec.dat target value'] = spec_dat.iat[0,1]\n",
    "glob = glob(os.path.normpath(os.path.join(ecotest_data_path, 'output', '*w_best_and_ssoln.dbf')))\n",
    "best_results = glob[0]\n",
    "best_results_df = pd.read_csv(best_results)\n",
    "out_sum_df['total extent'] = best_results['amount'].sum()\n",
    "out_sum_df['selected amount'] = [best_results['SOLUTION'] == 1, 'amount'].sum()\n",
    "current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "out_sum_df['Current_IUCN_TH'] = current_iucn_th\n",
    "out_sum_df['KBA m2 @ Current_IUCN_TH'] = (eco_subset_df.at[eco,'US_km2']*1000000)*current_iucn_th   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b753c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssolnimage_pdf_path = os.path.normpath(os.path.join(\n",
    "    testrun_basename + '_combined_ssoln_plots.pdf'))\n",
    "ssolnimage_list = ssolnimage_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac95c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestimage_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "446.844px",
    "left": "1964px",
    "right": "20px",
    "top": "34px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
