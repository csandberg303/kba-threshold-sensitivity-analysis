{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8fb7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "import requests\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# from qgis.core import *\n",
    "\n",
    "import contextily as cx\n",
    "import earthpy as et\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.plot import plotting_extent\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18781b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code including the 'create_input_dat' function and 'formatAsME'\n",
    "# function are taken/adapted from QMARXAN TOOLBOX ALGORITHM CODE\n",
    "\n",
    "# formatAsME - format as Marxan Exponent format like Input File Editor\n",
    "# (lines ~98-104 of QMarxan algorithm)\n",
    "def formatAsME(inVal):\n",
    "    outStr = \"%.14E\" % float(inVal)\n",
    "    parts = outStr.split('E')\n",
    "    sign = parts[1][:1]\n",
    "    exponent = \"%04d\" % float(parts[1][1:])\n",
    "    outStr = parts[0] + 'E' +  sign + exponent\n",
    "    return(outStr)\n",
    "\n",
    "\n",
    "# To create input.dat file (lines 128-183 of Qmarxan algorithm)\n",
    "def create_input_dat(dest, blm, numreps, numitns, runmode, heurtype, scen_id):\n",
    "    \"\"\"\n",
    "    To create the input.dat file that stores processing parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dest : str\n",
    "    directory input.dat file will be saved to\n",
    "     \n",
    "    blm : int\n",
    "    boundary length modifier\n",
    "    \n",
    "    numreps : int\n",
    "    Num of repeat runs (or solutions)\n",
    "    \n",
    "    numitns : int\n",
    "    Num of iterations for annealing\n",
    "    \n",
    "    runmode : int\n",
    "    number representing the annealing/heuristic options for a marxan run;\n",
    "    tells Marxan which method it should use to find the best reserve system \n",
    "    (i.e. simulated annealing, heuristic, or both). \n",
    "    \n",
    "    heurtype : int\n",
    "    value of 1 specifies that heuristic algorithm #1(Greedy) should be used if \n",
    "    RUNMODE 3 is selected, value of -1 specifies no heuristics will be used \n",
    "     \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output files\n",
    "     \n",
    "    other parameters will be added to replace the default initial values \n",
    "    that are included in the QMarxan code \n",
    "\n",
    "    -------\n",
    "    returned_data : the input.dat file \n",
    "\n",
    "    \"\"\"\n",
    "    output = os.path.join(dest,'input.dat')\n",
    "    f = open(output, 'w')\n",
    "    f.write(\"Input file for Annealing program.\\n\")\n",
    "    f.write('\\n')\n",
    "    f.write('This file generated for KBA Threshold\\n')\n",
    "    f.write('Analysis project using code from\\n')\n",
    "    f.write('QMarxan Toolbox 2.0\\n')\n",
    "    f.write('created by Apropos Information Systems Inc.\\n')\n",
    "    f.write('\\n')\n",
    "    f.write(\"General Parameters\\n\")\n",
    "    f.write(\"VERSION 0.1\\n\")\n",
    "    f.write(\"BLM %s\\n\" % formatAsME(blm)) # Boundary Length Modifier\n",
    "    f.write(\"PROP  %s\\n\" % formatAsME(0.5)) # Proportion of PU selected 1st run ??? is this where RUNMODE3 gets the selection (not from spec.dat)\n",
    "    f.write(\"RANDSEED -1\\n\") # Random seed number\n",
    "#     f.write(\"BESTSCORE  %s\\n\" % formatAsME(-1.0)) # NEW LINE SEEN IN NS SAMPLE ***** \n",
    "    f.write(\"NUMREPS \" + str(numreps) + \"\\n\") # Num of repeat runs (or solutions) ORIG USED 100 *****\n",
    "    f.write('\\n')\n",
    "    f.write(\"Annealing Parameters\\n\")\n",
    "    f.write(\"NUMITNS \" + str(numitns) + \"\\n\") # Num of iterations for annealing (ORIG VAL 1000000, changed to 10 which was unsuccessful in both RUNMODE 1 & 3) *****\n",
    "    f.write(\"STARTTEMP  %s\\n\" % formatAsME(-1.0)) # start temp for annealing\n",
    "    f.write(\"COOLFAC  %s\\n\" % formatAsME(-1.0)) # ***** (ORIG USED -1, changed to 6 to match inedit sample) cooling factor for annealing\n",
    "    f.write(\"NUMTEMP 10000\\n\") # num of temp decreases for annealing\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Cost Threshold\\n\")\n",
    "    f.write(\"COSTTHRESH %s\\n\" % formatAsME(0.0)) # cost threshold\n",
    "    f.write(\"THRESHPEN1 %s\\n\" % formatAsME(0.0)) # size of cost thresh penalty\n",
    "    f.write(\"THRESHPEN2 %s\\n\" % formatAsME(0.0)) # shp of cost thresh penalty\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Input Files\\n\")\n",
    "    f.write(\"INPUTDIR input\\n\") # name of dir containing input files\n",
    "    f.write(\"SPECNAME spec.dat\\n\") # Conservation Feature File\n",
    "    f.write(\"PUNAME pu.dat\\n\") # Planning Unit File\n",
    "    f.write(\"PUVSPRNAME puvsp.dat\\n\") # PU vs Conservation Feature File\n",
    "    f.write(\"BOUNDNAME bound.dat\\n\") # Boundary Length File\n",
    "#     f.write(\"BLOCKDEFNAME blockdef.dat\\n\") # Block Definition File\n",
    "#     f.write(\"MATRIXSPORDERNAME puvsp_sporder.dat\\n\") # PUVSPR ordered by SP\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Save Files\\n\")\n",
    "    f.write(\"SCENNAME \" + scen_id + \"\\n\") # Scenario name for saved output\n",
    "    f.write(\"SAVERUN 0\\n\") # Save each run (1-.dat, 2-.txt, 3-.csv)\n",
    "    f.write(\"SAVEBEST 3\\n\") # Save the best run (1-.dat, 2-.txt, 3-.csv)\n",
    "    f.write(\"SAVESUMMARY 3\\n\") # Save summary info (1-.dat, 2-.txt, 3-.csv)\n",
    "    f.write(\"SAVESCEN 3\\n\") # Save scenario info (1-.dat, 2-.txt, 3-.csv)\n",
    "    f.write(\"SAVETARGMET 3\\n\") # Save targets met information\n",
    "    f.write(\"SAVESUMSOLN 3\\n\") # Save summed solution info (1 dat,2 txt,3 csv)\n",
    "#     f.write(\"SAVEPENALTY 3\\n\") # Save computed feature penalties *****\n",
    "    f.write(\"SAVELOG 3\\n\") # Save log files (1-.dat, 2-.txt, 3-.csv)\n",
    "    f.write(\"SAVESNAPSTEPS 0\\n\") # Save snapshots of each n steps\n",
    "    f.write(\"SAVESNAPCHANGES 0\\n\") # Save snapshots after every n change\n",
    "    f.write(\"SAVESNAPFREQUENCY 0\\n\") # Frequency of snapshots if used\n",
    "    f.write(\"SAVESOLUTIONS MATRIX 3\\n\") # Save all runs in a single matrix\n",
    "    f.write(\"OUTPUTDIR output\\n\") # name of dir containing output files\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Program control.\\n\")\n",
    "    f.write(\"RUNMODE \" + str(runmode) + \"\\n\") # Run option *****\n",
    "    f.write(\"MISSLEVEL %s\\n\" % formatAsME(1.0)) # Species missing proportion\n",
    "    f.write(\"ITIMPTYPE 0\\n\") # Iterative improvement (ORIG USED 1) *****\n",
    "    f.write(\"HEURTYPE \" + str(heurtype) + \"\\n\") # Heuristic ??? add to input parameters, or change to 1(Greedy) if RUNMODE 3\n",
    "    f.write(\"CLUMPTYPE 2\\n\") # Clumping rule 2. Graduated penaltyâ€“ Score is proportional to the size of the clump *****\n",
    "    f.write(\"VERBOSITY 3\\n\") # Screen output\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    print(os.path.basename(dest) + \": input.dat created successfully\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ad2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write formula to get the shapefile and rasters that have been saved to the \n",
    "#''kba_thres_sa/shp_hex' and 'kba_thres_sa/r_tif' local directories\n",
    "\n",
    "# Currently I've manually copied Lana's ArcGIS files to these locations, using \n",
    "# the naming convention 'eco.shp' for hexfiles and 'eco.tif for the rasters.\n",
    "\n",
    "# IN THE FUTURE, the .shp & .tif files may be created and placed in the \n",
    "# 'shp_hex' and 'r_tif' directories using code rather than ArcGIS, but this \n",
    "# 'get_source_files' formula will still function to copy the needed files into\n",
    "# the 'eco' directory when the 'eco/input' directories are created.\n",
    "\n",
    "def get_source_files(path, eco, scen_id):\n",
    "    \"\"\"\n",
    "    path : str\n",
    "    local directory where the shapefiles or rasters are stored\n",
    "    \n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "     \n",
    "    \"\"\"\n",
    "    source_file_ls = glob(os.path.join(path, eco + '*'))\n",
    "    if source_file_ls == []:\n",
    "        print(\"no files found in \" + path + \"with expected name \" + eco + \"?\")\n",
    "    else:\n",
    "        for file in source_file_ls:\n",
    "            shutil.copy(file, os.getcwd())\n",
    "            print(scen_id + \": \"+ os.path.basename(file) + \n",
    "                  \" copied successfully\")\n",
    "         \n",
    "    return print(scen_id + \": finished copying source files from \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9ab349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create pu.dat file \n",
    "\n",
    "def create_pu_dat(eco, path, scen_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    To create the pu.dat file that stores information about planning units in \n",
    "    hex grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    path : str\n",
    "    local directory where 'hex_shp' directory is stored\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "     \n",
    "    -------\n",
    "    returned_data : the pu.dat input file \n",
    "         \n",
    "    \"\"\"\n",
    "    source_data_path = os.path.join(path, 'source_data')\n",
    "\n",
    "    # open hex.shp file with set crs\n",
    "    shp_path = glob(os.path.join(source_data_path, eco + '.shp'))   \n",
    "    \n",
    "    # create df based on hexfile.shp\n",
    "    shp_layer = gpd.read_file(shp_path[0])\n",
    "        \n",
    "    # create new column in .shp for 'id'    \n",
    "    shp_layer.insert(0, 'id', range(1, 1 + len(shp_layer)))\n",
    "    \n",
    "    # set values in column 'Cost' to 1, and column 'Status' to = 0\n",
    "    shp_layer[\"cost\"] = 1\n",
    "    shp_layer[\"status\"] = 0\n",
    "    \n",
    "    # create pu.dat file\n",
    "    pu_dat = shp_layer[[\"id\", \"cost\", \"status\"]].set_index(\"id\")\n",
    "    output = pu_dat.to_csv('pu.dat')\n",
    "    print(scen_id + \": pu.dat file created successfully\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515f3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function (v1) to create spec.dat file (w/o target2, if minclump set to False)\n",
    "\n",
    "def create_spec_dat_v1(info_df, eco, prop, spf, scen_id):\n",
    "    \"\"\"\n",
    "    To create the spec.dat file, which stores information about ecosytem to be \n",
    "    analyzed in marxan run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info_df : df\n",
    "    dataframe of ecosystem info, including 'Short_Name', 'US_km2' and\n",
    "    'Current_IUCN_TH' columns\n",
    "     \n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    prop : float\n",
    "    The proportion of the total amount of the feature which must be included \n",
    "    in the solution; must be between 0 and 1 (tutorial suggests 0.3)\n",
    "     \n",
    "    spf : int\n",
    "    species penalty factor\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "  \n",
    "    -------\n",
    "     \n",
    "    returned_data : the spec.dat input file (without a 'target2' column)\n",
    "         \n",
    "    \"\"\"\n",
    "    # set columns of spec.dat, if minclump parameter is False\n",
    "    data = [{'id': 1, 'prop': prop, 'spf': spf, 'name': eco}]\n",
    "   \n",
    "    # set index, and save file as 'spec.dat'\n",
    "    spec_dat = pd.DataFrame(data).set_index('id')\n",
    "    output = spec_dat.to_csv('spec.dat')\n",
    "    print(scen_id + \": spec.dat file created successfully (v1)\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4abab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd try - funtion to create spec.dat file (incl target2 and prop)\n",
    "\n",
    "def create_spec_dat_v2(info_df, prop, target2, spf, eco, scen_id):\n",
    "    \"\"\"\n",
    "    To create the spec.dat file, which stores information about ecosytem to be \n",
    "    analyzed in marxan run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info_df : df\n",
    "    dataframe of ecosystem info, including 'Short_Name', 'US_km2' and\n",
    "    'Current_IUCN_TH' columns\n",
    "        \n",
    "    prop : float\n",
    "    The proportion of total ecosystem area that must be included in solution\n",
    "    \n",
    "    target2: float\n",
    "    minimum clumpsize of area, in order to be included in solution (*KBA*)\n",
    "    \n",
    "    spf : int\n",
    "    species penalty factor\n",
    "    \n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "    -------\n",
    "     \n",
    "    returned_data : the spec.dat input file \n",
    "         \n",
    "    \"\"\"\n",
    "    data = [{'id': 1, \n",
    "             'prop': prop,\n",
    "             'spf': spf, \n",
    "             'target2': target2, \n",
    "             'name': eco\n",
    "            }]    \n",
    "    # set index, and save file as 'spec.dat'\n",
    "    spec_dat = pd.DataFrame(data).set_index('id')\n",
    "    output = spec_dat.to_csv('spec.dat')\n",
    "    print(scen_id + \": spec.dat file created successfully (v2)\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2f5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd try - funtion to create spec.dat file (with target2 and target)\n",
    "\n",
    "def create_spec_dat_v3(info_df, target, target2, spf, eco, scen_id):\n",
    "    \"\"\"\n",
    "    To create the spec.dat file, which stores information about ecosytem to be \n",
    "    analyzed in marxan run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info_df : df\n",
    "    dataframe of ecosystem info, including 'Short_Name', 'US_km2' and\n",
    "    'Current_IUCN_TH' columns\n",
    "        \n",
    "    target : float\n",
    "    The total ecosystem area that must be included in solution\n",
    "    \n",
    "    target2: float\n",
    "    minimum clumpsize of area, in order to be included in solution (*KBA*)\n",
    "    \n",
    "    spf : int\n",
    "    species penalty factor\n",
    "    \n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "    -------\n",
    "     \n",
    "    returned_data : the spec.dat input file \n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    data = [{'id': 1, \n",
    "             'target': target,\n",
    "             'spf': spf, \n",
    "             'target2': target2, \n",
    "             'name': eco\n",
    "            }]  \n",
    "    \n",
    "    # set index, and save file as 'spec.dat'\n",
    "    spec_dat = pd.DataFrame(data).set_index('id')\n",
    "    output = spec_dat.to_csv('spec.dat')\n",
    "    print(scen_id + \": spec.dat file created successfully (v3)\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7814d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function (v4) to create spec.dat file (w target only)\n",
    "\n",
    "def create_spec_dat_v4(info_df, eco, target, spf, scen_id):\n",
    "    \"\"\"\n",
    "    To create the spec.dat file, which stores information about ecosytem to be \n",
    "    analyzed in marxan run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info_df : df\n",
    "    dataframe of ecosystem info, including 'Short_Name', 'US_km2' and\n",
    "    'Current_IUCN_TH' columns\n",
    "     \n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed; \n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "    \n",
    "    target : float\n",
    "    the total amount of the feature which must be included \n",
    "    must be in same UOM as 'amount' in puvsp\n",
    "     \n",
    "    spf : int\n",
    "    species penalty factor\n",
    "    \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "  \n",
    "    -------\n",
    "     \n",
    "    returned_data : the spec.dat input file (without a 'target2' column)\n",
    "         \n",
    "    \"\"\"\n",
    "    # set columns of spec.dat, if minclump parameter is False\n",
    "    data = [{'id': 1, 'target': target, 'spf': spf, 'name': eco}]\n",
    "   \n",
    "    # set index, and save file as 'spec.dat'\n",
    "    spec_dat = pd.DataFrame(data).set_index('id')\n",
    "    output = spec_dat.to_csv('spec.dat')\n",
    "    print(scen_id + \": spec.dat file created successfully (v4)\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3518749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get Lana's input files (created with ArcGIS) from the \n",
    "# repo to local directory \n",
    "def get_marxan_input_files(eco, files_to_get, scen_id):\n",
    "     \"\"\"\n",
    "     Currently this formula will find the input files Lana created using the\n",
    "     ArcMarxan Toolbox plugin in ArcGIS, which have been stored to the assets\n",
    "     directory of our GitHub repository.  We hope this may be a placeholder\n",
    "     function, to be replaced with functions that might create these files \n",
    "     directly using the opensource code available from the opensource QMarxan \n",
    "     Toolbox plugin for QGIS.\n",
    "\n",
    "     Parameters\n",
    "     ----------\n",
    "     eco : str\n",
    "     the abbreviated one word short name used for ecosystem being analyzed; \n",
    "     identifies a subdirectory of the timestamped marxan run directory\n",
    "     \n",
    "     files_to_get : list\n",
    "     list of filenames to retrieve from the marxan_input/eco directory of \n",
    "     the repo\n",
    "\n",
    "     -------\n",
    "     returned_data : the specified dat files, saved to eco/input local \n",
    "     directory\n",
    "     \"\"\"\n",
    "     inputfile_ls = files_to_get\n",
    "     \n",
    "     for file in inputfile_ls:\n",
    "        urltext = (\"https://raw.githubusercontent.com/csandberg303/\"\n",
    "                   \"kba-threshold-sensitivity-analysis/main/assets/data/\"\n",
    "                   \"marxan_input/\")\n",
    "        url = urltext + eco + \"/\" + file\n",
    "        \n",
    "        #test new method to get file from url\n",
    "#         df = pd.read_csv(url, sep='\\t')\n",
    "        df = pd.read_csv(url, sep = None, engine = 'python')\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # downloading the info from file stored on github\n",
    "#         fileinfo = requests.get(url).content\n",
    "#         # Reading the downloaded content and turning it to a pandas dataframe\n",
    "#         fileinfo_df = pd.read_csv(io.StringIO(fileinfo.decode('utf-8')),\n",
    "#                                  index_col=False)#.squeeze(\"columns\")\n",
    "#         filename = file\n",
    "        df.to_csv(file, index=False)\n",
    "#         np.savetxt(file + '.dat', fileinfo_df, delimiter=',')\n",
    "        print(scen_id + \": \" + file + \" successfully copied from url\")\n",
    "     return \n",
    "    \n",
    "\n",
    "\n",
    "#test new method to get file from url\n",
    "# df = pd.read_csv(url, sep='\\t')#.squeeze()\n",
    "\n",
    "# df.info()\n",
    "\n",
    "\n",
    "# # #         # downloading the info from file stored on github\n",
    "# # #         fileinfo = requests.get(url).content\n",
    "# # #         # Reading the downloaded content and turning it to a pandas dataframe\n",
    "# # #         fileinfo_df = pd.read_csv(io.StringIO(fileinfo.decode('utf-8')),\n",
    "# # #                                  index_col=False)#.squeeze(\"columns\")\n",
    "# # filename = file\n",
    "# df.to_csv('testbound.dat', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3d0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set crs of shp and tif to ESPG 5070 and save as new files, then use to \n",
    "# create output plots based on 'best_run' and 'summed_solutions'\n",
    "\n",
    "def get_output_plots(path, eco, espg, target2, scen_id):\n",
    "\n",
    "  \n",
    "    \"\"\"\n",
    "    To set crs of shp and tif to ESPG 5070, add columns to shp and save as new\n",
    "    files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "    filepath to ecotest subdirectory\n",
    "\n",
    "    eco : str\n",
    "    the abbreviated one word short name used for ecosystem being analyzed;\n",
    "    identifies a subdirectory of the timestamped marxan run directory\n",
    "\n",
    "    espg : str\n",
    "    espg number (we're using ESPG:5070)\n",
    "\n",
    "    target2: float\n",
    "    minimum clumpsize of area, in order to be included in solution (*KBA*)\n",
    "\n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output\n",
    "\n",
    "    -------\n",
    "    returned_data : updated shp and tif (and more????)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # first test for output files, to see if run completed successfully\n",
    "    # open '_best' file created by Marxan and saved to 'output' directory\n",
    "    globfile_best = glob(os.path.normpath(os.path.join(path, 'output', \n",
    "                                                       '*_best*')))\n",
    "\n",
    "    if globfile_best == []:\n",
    "        output = print (scen_id + \": ERROR: best run file not found - check \"\n",
    "                        \"output/log. \\nWill need to resolve error and rerun \"\n",
    "                        \"Marxan if final output files have not completed \"\n",
    "                        \"successfully\")\n",
    "    else:\n",
    "    \n",
    "        # open the shp file saved at 'path/source_data' location\n",
    "        shp_data_path = os.path.join(path, \"source_data\", eco + '.shp')\n",
    "        shp_layer = gpd.read_file(shp_data_path)\n",
    "        # reproject CRS of shp\n",
    "        shp_layer_crs = shp_layer.to_crs(epsg=espg)\n",
    "        # create new .shp file\n",
    "    #     shp_espg_file =\n",
    "        shp_layer_crs.to_file(eco + \"_espg_\" + espg + \".shp\",\n",
    "                                              index=False)\n",
    "        shp_layer_crs_path = os.path.join(os.getcwd(), \n",
    "                                          eco + \"_espg_\" + espg +'.shp')\n",
    "\n",
    "        if os.path.exists(shp_layer_crs_path):\n",
    "            print(scen_id + ': PU shapefile reprojected to ESPG: ' + espg + \n",
    "                  \" and saved to 'source_data'\")\n",
    "        else:\n",
    "            print(scen_id + (\": Error: reprojected shapefile was not able to\"\n",
    "                  \"be saved\"))\n",
    "\n",
    "        # open the tif file saved at 'path/source_data' location\n",
    "        tif_data_path = os.path.join(path, \"source_data\", eco + '.tif')\n",
    "        tif_layer = rxr.open_rasterio(tif_data_path, masked=True).squeeze()\n",
    "        # reproject CRS of tif; first create a rasterio crs object\n",
    "        crs_espg = CRS.from_string('EPSG:' + espg)\n",
    "        # then reproject tif using the crs object\n",
    "        tif_layer_crs = tif_layer.rio.reproject(crs_espg)\n",
    "        # create path that new tif file will be saved to\n",
    "        tif_layer_crs_path = os.path.join(os.getcwd(),\n",
    "                                          eco + \"_espg_\" + espg + \".tif\")\n",
    "        # create new .tif file\n",
    "    #     tif_espg_file =\n",
    "        tif_layer_crs.rio.to_raster(tif_layer_crs_path)\n",
    "\n",
    "        if os.path.exists(tif_layer_crs_path):\n",
    "            print(scen_id + ': Raster reprojected to ESPG: ' + espg + \n",
    "                  \" and saved to 'source_data'\")\n",
    "        else:\n",
    "            print(scen_id + (\": Error: reprojected raster was not able to be \"\n",
    "                             \"saved\"))\n",
    "\n",
    "        # define raster extent for plotting\n",
    "        raster_extent = plotting_extent(tif_layer_crs,\n",
    "                                        tif_layer_crs.rio.transform())\n",
    "        \n",
    "        ###\n",
    "        # Open raster data, set plotting extent\n",
    "    #     raster_path = os.path.normpath(os.path.join(path,\n",
    "    #                                \"source_data\",\n",
    "    #                                eco + \"_espg_\" + espg + \".tif\"))\n",
    "    #     raster_layer = rxr.open_rasterio(raster_path, masked=True).squeeze()\n",
    "\n",
    "\n",
    "    #     # open shapefile created in the 'set_source_files_crs' function\n",
    "    #     shp_path = os.path.normpath(os.path.join(\n",
    "    #         path, \"source_data\", eco + \"_espg_\" + espg + \".shp\"))\n",
    "    #     shp_layer = gpd.read_file(shp_path)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        best_run_path = globfile_best[0]\n",
    "        best_run = pd.read_csv(best_run_path)\n",
    "\n",
    "        # merge best_run df to shp layer\n",
    "        shp_layer_crs.insert(0, 'PUID', range(1, 1 + len(shp_layer_crs)))\n",
    "        shp_layer_crs = shp_layer_crs.merge(best_run, on='PUID')\n",
    "\n",
    "        # open 'puvsp.dat' and merge with shp layer to get 'amount' from puvsp\n",
    "        puvsp_path = os.path.normpath(os.path.join(path, 'input', \n",
    "                                                   'puvsp.dat'))\n",
    "        puvsp = pd.read_csv(puvsp_path)\n",
    "        puvsp = puvsp.rename(columns={'pu': 'PUID'})\n",
    "        shp_layer_crs = shp_layer_crs.merge(puvsp, on='PUID')\n",
    "\n",
    "        best_fig_title_metric = shp_layer_crs.query(\n",
    "            \"SOLUTION == 1\")['amount'].sum()#/1000000\n",
    "        best_ftm_string = str(best_fig_title_metric)\n",
    "        \n",
    "        # get total extent of ecosystem (???from the amount column, orig in the puvsp)\n",
    "        eco_extent = shp_layer_crs.query(\"SOLUTION < 2\")['amount'].sum()#/1000000\n",
    "#         df.at[eco,'US_km2']*df.at[eco,'Current_IUCN_TH'])) + \" km2\\n\")\n",
    "\n",
    "        # save merged shp as new file\n",
    "        shp_w_best_and_amt = shp_layer_crs.to_file(eco + \"_w_best.shp\", \n",
    "                                                   index=False)\n",
    "        print (scen_id + ': ' + eco + '.shp merged with ' + scen_id +\n",
    "               \"_best.csv and puvsp.dat, saved as \" + eco +\n",
    "               \"_w_best_and_amt.shp file\")\n",
    "        print ('preparing plots...')\n",
    "\n",
    "        # create visualization showing hexcell selection from best run solution\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "        shp_layer_crs.plot(column='SOLUTION', cmap='viridis', ax=ax, alpha=0.65)\n",
    "\n",
    "        ax.set(title= scen_id + ': best run solution' + \n",
    "               \"\\nTotal Extent: \" + str(eco_extent) + ' sq m' +\n",
    "               '\\ntarget2 = ' + str(target2) + ' sq m' + \n",
    "               '\\nTotal Selected Ecoystem = ' + best_ftm_string + ' sq m'\n",
    "              )\n",
    "        ax.set_axis_off()\n",
    "        cx.add_basemap(ax=ax, crs=shp_layer.crs)\n",
    "        ax.imshow(tif_layer_crs, cmap='jet', extent=raster_extent,\n",
    "          interpolation='nearest')\n",
    "        plt.savefig((scen_id + ' best_plot.png'), facecolor='w', \n",
    "                    edgecolor='k', dpi=1200)\n",
    "        plt.close(fig)\n",
    "        print (scen_id + \": best plot saved as .png\\n\")\n",
    "\n",
    "    ###\n",
    "\n",
    "    # open '_ssoln' file created by Marxan and saved to 'output' directory\n",
    "    globfile_ssoln = glob(os.path.normpath(os.path.join(path, 'output',\n",
    "                                                        '*_ssoln*')))\n",
    "    if globfile_ssoln == []:\n",
    "        output = print (scen_id + \": ERROR: ssoln file not found - check \"\n",
    "                        \"output/log. \\nWill need to resolve error and rerun \"\n",
    "                        \"Marxan if final output files have not completed \"\n",
    "                        \"successfully\")\n",
    "    else:\n",
    "        ssoln_path = globfile_ssoln[0]\n",
    "        ssoln = pd.read_csv(ssoln_path)\n",
    "        ssoln = ssoln.rename(columns={'planning_unit': 'PUID'})\n",
    "\n",
    "        # merge ssoln df to shp layer\n",
    "#         shp_layer_crs.insert(0, 'PUID', range(1, 1 + len(shp_layer_crs)))\n",
    "        shp_layer_crs = shp_layer_crs.merge(ssoln, on='PUID')\n",
    "        shp_layer_crs.to_file(eco + \"_w_best_and_ssoln.shp\", index=False)\n",
    "        \n",
    "        merged_shp_layer_path = os.path.normpath(\n",
    "            os.path.join(os.getcwd(), eco + \"_w_best_and_ssoln.shp\"))\n",
    "        \n",
    "\n",
    "        if os.path.exists(merged_shp_layer_path):\n",
    "            print(scen_id + (\": Shapefile merged with best, puvsp and ssoln, \"\n",
    "                             \"and saved to 'source_data'\"))\n",
    "        else:\n",
    "            print(scen_id + \": merged shapefile was not able to be saved\")\n",
    "\n",
    "        \n",
    "        \n",
    "        print (eco + '.shp merged with ' + scen_id + \"puvsp.dat, best.csv and\"\n",
    "               \" ssoln.csv, saved as \" + scen_id +\n",
    "               \"_w_best_and_ssoln.shp file\")\n",
    "        print ('preparing plots...')\n",
    "\n",
    "        # create visualization showing hexcell selection from summed solution\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 10))\n",
    "        shp_layer_crs.plot(column='number', cmap='viridis', ax=ax2, alpha=0.65)\n",
    "        ax2.imshow(tif_layer_crs, cmap='jet', extent=raster_extent,\n",
    "                  interpolation='nearest')\n",
    "        ax2.set(title= scen_id + ': summed solution' +\n",
    "               '\\n(hex cell selection frequency)')\n",
    "        ax2.set_axis_off()\n",
    "        cx.add_basemap(ax2, crs=shp_layer.crs)\n",
    "\n",
    "        plt.savefig((scen_id + 'ssoln_plot.png'), facecolor='w', \n",
    "                    edgecolor='k', dpi=1200)\n",
    "        plt.close(fig2)\n",
    "        print (scen_id + \": ssoln plot saved as .png\\n\")\n",
    "\n",
    "#         output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accd1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CREATE summary of the marxan run, incl info from the output file scenario \n",
    "# details (sen.dat), input variables within the workflow (ALSO ADD AREA & \n",
    "# OUTPUT STATS)\n",
    "def create_mxrun_summary(dest, espg, prop, blm, target2, spf, scen_id, eco, df):\n",
    "    \"\"\"\n",
    "    To create an output summary, showing local variables and info from sen.dat \n",
    "    output file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dest : str\n",
    "    path to the 'eco' subdirectory \n",
    "    \n",
    "    espg : str\n",
    "    espg number (we're using ESPG:5070)\n",
    "   \n",
    "    prop : float - USE BLM HERE INSTEAD\n",
    "    The proportion of the total amount of the feature which must be included \n",
    "    in the solution; must be between 0 and 1 (tutorial suggests 0.3)\n",
    "    \n",
    "    target2 : float\n",
    "    the min acceptable clump size - SHOULD EQUAL KBA (5 or 10 % x test thresh)\n",
    "     \n",
    "    spf : int\n",
    "    species penalty factor\n",
    "     \n",
    "    scen_id : str\n",
    "    scenario id, info to be included as prefix on generated output files\n",
    "    \n",
    "    df : df \n",
    "    provided df with info about ecosystem's RLE status and area extent\n",
    "     \n",
    "    other parameters may be added to replace the default initial values \n",
    "    that are included in the QMarxan code \n",
    "\n",
    "    -------\n",
    "    returned_data : the input.dat file \n",
    "         \n",
    "    \"\"\"\n",
    "    # display info from sen.dat output file\n",
    "    sen_path = glob(os.path.normpath(os.path.join(dest, \"output\", \"*_sen.*\")))\n",
    "    sen_df = pd.read_table(sen_path[0], header=None)\n",
    "    sen_l1 = sen_df[0].iloc[0]\n",
    "    sen_l2 = sen_df[0].iloc[1]\n",
    "    sen_l3 = sen_df[0].iloc[2]\n",
    "    sen_l4 = sen_df[0].iloc[3]\n",
    "    sen_l5 = sen_df[0].iloc[4]\n",
    "    sen_l6 = sen_df[0].iloc[5]\n",
    "    sen_l7 = sen_df[0].iloc[6]\n",
    "    sen_l8 = sen_df[0].iloc[7]\n",
    "    sen_l9 = sen_df[0].iloc[8]\n",
    "    sen_l10 = sen_df[0].iloc[9]\n",
    "    sen_l11 = sen_df[0].iloc[10]\n",
    "    sen_l12 = sen_df[0].iloc[11]\n",
    "    sen_l13 = sen_df[0].iloc[12]\n",
    "    sen_l14 = sen_df[0].iloc[13]\n",
    "    sen_l14 = sen_df[0].iloc[14]\n",
    "    sen_l15 = sen_df[0].iloc[15]\n",
    "    \n",
    "    output = os.path.join(dest, 'output', scen_id + '_mxrun_summary.dat')\n",
    "    f = open(output, 'w')\n",
    "    f.write(dest + '\\n')\n",
    "    f.write(\"Scenario Details\\n\")\n",
    "    f.write(sen_l1 + \"\\n\")\n",
    "    f.write(sen_l2 + \"\\n\")\n",
    "    f.write(sen_l3 + \"\\n\")\n",
    "    f.write(sen_l4 + \"\\n\")\n",
    "    f.write(sen_l5 + \"\\n\")\n",
    "    f.write(sen_l6 + \"\\n\")\n",
    "    f.write(sen_l7 + \"\\n\")\n",
    "    f.write(sen_l8 + \"\\n\")\n",
    "    f.write(sen_l9 + \"\\n\")\n",
    "    f.write(sen_l10 + \"\\n\")\n",
    "    f.write(sen_l11 + \"\\n\")\n",
    "    f.write(sen_l12 + \"\\n\")\n",
    "    f.write(sen_l13 + \"\\n\")\n",
    "    f.write(sen_l14 + \"\\n\")\n",
    "    f.write(sen_l15 + \"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "     # display info from stored variables in workflow\n",
    "    blmstr = str(blm)\n",
    "    propstr = str(prop)\n",
    "    tgt2str = str(target2)\n",
    "    spfstr = str(spf)\n",
    "#     beststr = str(best_fig_title_metric)\n",
    "    \n",
    "    f.write('Variables Set Locally -\\n')\n",
    "    f.write('scen_id: ' + scen_id + '\\n')\n",
    "    f.write('ESPG value for raster and shapefile: ' + espg + \"\\n\")\n",
    "#     f.write('input file variables:\\n')\n",
    "    f.write('Boundary Length Modifier (BLM): ' +  blmstr + \"\\n\")\n",
    "    f.write('Species Penalty Factor (SPF): ' + spfstr +'\\n')\n",
    "    f.write(\"\\n\")\n",
    "    f.write('prop: ' +  propstr + \"\\n\")\n",
    "    f.write('target2: ' +  tgt2str + \" (\" + str(target2/1000000) + \" km2)\\n\")\n",
    "#     f.write('best_run fig title metric' + beststr + \" km2)\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # display data from eco df\n",
    "    f.write('Spatial Extent of Ecosystem & KBA Thresholds\\n')\n",
    "    f.write('eco: ' + eco + '\\n')\n",
    "    f.write('US_km2: ' + str(df.at[eco,'US_km2']) + '\\n')\n",
    "    f.write('RLE_FINAL: ' + df.at[eco,'RLE_FINAL'] + '\\n')\n",
    "    f.write('Current_IUCN_TH: ' + str(df.at[eco,'Current_IUCN_TH']) + '\\n') \n",
    "    f.write('KBA @ 1.00 IUCN TH: ' + str(df.at[eco,'US_km2']*df.at[eco,'Current_IUCN_TH']) + \" km2\\n\")\n",
    "    f.write('KBA @ 0.75 IUCN TH: ' + str(0.75*(df.at[eco,'US_km2']*df.at[eco,'Current_IUCN_TH'])) + \" km2\\n\")\n",
    "    f.write('KBA @ 0.50 IUCN TH: ' + str(0.50*(df.at[eco,'US_km2']*df.at[eco,'Current_IUCN_TH'])) + \" km2\\n\")\n",
    "    f.write('KBA @ 0.25 IUCN TH: ' + str(0.25*(df.at[eco,'US_km2']*df.at[eco,'Current_IUCN_TH'])) + \" km2\\n\")\n",
    "    f.close()\n",
    "    print(os.path.basename(dest) + ': mxrunsummary created successfully\\n')\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
