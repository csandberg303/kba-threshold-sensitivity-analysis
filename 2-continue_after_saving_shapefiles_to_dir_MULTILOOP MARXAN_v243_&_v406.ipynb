{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3432df2c",
   "metadata": {},
   "source": [
    "##### The workflow continues in this notebook, where three main actions occur\n",
    "* Marxan input files and directories are prepared\n",
    "* the Marxan executable is called\n",
    "* Summary files and plots are created from the output of the Marxan run\n",
    "\n",
    "Some manual steps are needed to be taken by the user to set up the workflow \n",
    "components, uch as downloading the Marxan excutable and saving shapefiles and \n",
    "raster files created using other software to specifically named directories \n",
    "that were created by this workflow in the first notebook.  Requiring this sort of \n",
    "manual coordination may be addressed in the future, to provide more automation\n",
    "throughout the process.\n",
    "\n",
    "Once the setup steps have been completed, a few cells are offered to set variables \n",
    "that determine how the Marxan run will be completed. \n",
    "\n",
    "The bulk of the workflow occurs in the final cells of the notebook, which use loops \n",
    "both to control the Marxan analysis and also produce summary plots and tables.\n",
    "\n",
    "The summary output can be reviewed to determine how well it meets the intended goals, \n",
    "informing the user on possible adjustments to be made in the input variables until \n",
    "optimal results are acheived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2f48e",
   "metadata": {},
   "source": [
    "##### The first cells import libraries, and set the working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa221e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import io\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import contextily as cx\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.plot import plotting_extent\n",
    "import rioxarray as rxr\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "import kba_thresh_sa_scripts as ks\n",
    "\n",
    "# set global cache override variable\n",
    "CACHE_OVERRIDE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6fa16",
   "metadata": {},
   "source": [
    "##### Check if 'kba_thresh_sa' directory exists\n",
    "* If it does, change working directory to 'earth-analytics/data/kba_thresh_sa' \n",
    "* define paths to two directories that should have been created in first notebook\n",
    "    * 'hex_shp' directory for ecosystem shapefiles\n",
    "    * 'r_tif' directory for ecosystem rasters \n",
    "* verify the two directories are not empty\n",
    " \n",
    "* If directories are missing or empty, prompt user to return to first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filepath to working directory\n",
    "data_path = os.path.normpath(os.path.join(et.io.HOME, \n",
    "                                          'earth-analytics', \n",
    "                                          'data', \n",
    "                                          'kba_thresh_sa'))\n",
    "\n",
    "# Check if 'kba_thresh_sa' directory exists.  \n",
    "# If it does, make that the current working directory\n",
    "if os.path.exists(data_path):\n",
    "    print('Working directory set to earth-analytics/data/kba_thresh_sa.')\n",
    "    os.chdir(data_path)\n",
    "    \n",
    "    # define a path to the hexfiles directory created in the 1st notebook\n",
    "    shp_dir_path = os.path.normpath(os.path.join(data_path, 'hex_shp'))\n",
    "    # check to see if it exists\n",
    "    if os.path.exists(shp_dir_path):\n",
    "        # check to see if it contains files\n",
    "        dir = os.listdir(shp_dir_path)\n",
    "        if len(dir) == 0:\n",
    "            print(\"Empty 'hex_shp' directory, please add needed .shp files\"\n",
    "                  \"to \" + shp_data_path)\n",
    "    else:\n",
    "        print(\"'hex_shp' directory missing, please run 1st notebook \"\n",
    "              \"before proceeding\")\n",
    "            \n",
    "    # define a path to the raster directory created in the 1st notebook\n",
    "    tif_dir_path = os.path.normpath(os.path.join(data_path, 'r_tif'))\n",
    "    # check to see if it exists\n",
    "    if os.path.exists(tif_dir_path):\n",
    "        dir = os.listdir(tif_dir_path)\n",
    "        # check to see if it contains files\n",
    "        if len(dir) == 0:\n",
    "            print(\"Empty 'r_tif' directory, please add needed rasters to \" \n",
    "                  + tif_dir_path)\n",
    "    else:\n",
    "        print(\"'r_tif' directory missing, please run 1st notebook \"\n",
    "              \"before proceeding\")\n",
    "else:\n",
    "    print(\"Please go to first notebook in workflow to set up initial \"\n",
    "          \"directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f62d54",
   "metadata": {},
   "source": [
    "##### Marxan executable will need to be manually downloaded and and saved to 'earth-analytics/data/kba_thresh_sa'. \n",
    "A path to this location will be defined in the cell below; user should ensure \n",
    "the correct version is saved to the correct location with the correct name.\n",
    "\n",
    "if using \n",
    "* Marxan v2.43 (currently preferred): save to 'earth-analytics/data/kba_thresh_sa' \n",
    "dir with filename 'Marxan_x64_243.exe'\n",
    "* Marxan v4.06: save to 'earth-analytics/data/kba_thresh_sa' dir with filename \n",
    "'Marxan_x64_243.exe'\n",
    "\n",
    " [link to marxansolutions.org to download v4.06 and v2.43](https://marxansolutions.org/software//)\n",
    "\n",
    "\n",
    "##### A bit of background on 'target2'...\n",
    "\n",
    "Mutliple versions of Marxan have been developed since its inception in 2000, \n",
    "enabling the software to work with increasingly complex multi-species scenarios.\n",
    "It appears that as development occurred in some areas, other earlier features\n",
    "were no longer supported as a trade-off?  One such unsupported feature may be \n",
    "the 'target2' variable in the 'spec.dat' input file.  \n",
    "\n",
    "This variable was set to define a minimum size of a selected conservation area, \n",
    "and allowed for multiple selections of cells (known as 'clumps') to be generated \n",
    "in a single Marxan run in order to meet a larger conservation goal.  This \n",
    "functioned nicely to define individual KBAs of a minimum size that could be added \n",
    "together to reach a larger overall conservation target.  For example 'target2' \n",
    "could be set to the IUCN KBA Threshold size for a Vulnerable ecosytem, which is \n",
    "10% of the overall ecosystem extent.  The larger overall target would be set at\n",
    "30% of the total ecosystem extent. Setting the input parameters in this way would \n",
    "result in three clumps @ 10% size, which when added together would reach the \n",
    "30% conservation goal.\n",
    "\n",
    "Multiple attempts were made this during this course to find a way to work with \n",
    "'target2' using the more recent versions of Marxan, as these versions had more \n",
    "robust user manuals and seemed like they would be easier to work with. None of\n",
    "these attempts had been successful, with the Marxan output showing a succession\n",
    "of zeros instead of the more favorable calculated results.  \n",
    "\n",
    "In the end, two workarounds were considered.  One was to rework the input files \n",
    "and workflow to match the format required Marxan 1.8.10.  A second idea was to \n",
    "use while loops in the python workflow developed for Marxan v2.43/v4.06 in order \n",
    "to generate multiple marxan runs for a single test level (without using the \n",
    "'target2' variable in the 'spec.dat' input file).  This is what the current \n",
    "notebook is attempting to accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to Marxan.exe executable file has been manually copied over to \n",
    "# 'kba_thresh_sa' directory \n",
    "\n",
    "# v4.0.6 (might be causing 'target2' crash? use 2.43 instead)\n",
    "marxan_path = os.path.join(data_path, \"Marxan_x64.exe\")\n",
    "\n",
    "# v2.43 (spec.dat files with 'target2' field also crashed using v2.43, so it\n",
    "# was decided to stop using 'target2'. Workflow currently uses v2.43 in run)\n",
    "marxan_243_path = os.path.join(data_path, 'Marxan_x64_243.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976ca3c",
   "metadata": {},
   "source": [
    "##### A table of information about the ecosystems is provided   \n",
    "\n",
    "The workflow requires an associated table with information about the \n",
    "ecosystems to be analyzed. Currently our workflow uses the 'README' .csv file\n",
    "provided along with the Landfire EVT 2020 raster.  This file has been manually \n",
    "uploaded to our GitHub repo as 'Assets/Data/'from_LF_EVT_2020_README.csv'. \n",
    "The code below will download that file from URL to a pandas dataframe, and then \n",
    "also save that dataframe locally as a csv. \n",
    "\n",
    "For testing purposes, 9 ecosystem rows were filtered out of the 856 total rows \n",
    "included in the full file.  Three ecosystems are selected for the initial test, \n",
    "and their supporting shapefiles and rasters have been uploaded to the data \n",
    "directory on GitHub in two separate subdirectories (one for shapefiles and the \n",
    "other for rasters). Lana has generated the individual the shapefiles and rasters \n",
    "for the remaining six ecosystems included in the filtered README file using \n",
    "ArcGIS, but those files have not been uploaded to GitHub due to size constraints.\n",
    "\n",
    "All ~50 columns of the original README file were kept in the file uploaded to \n",
    "GitHub.  These provide a range of information about the 9 ecosystem records, \n",
    "including their status as determined by the IUCN Red List of Ecosystems (which \n",
    "in turn defines the minimum size for a KBA to be established). One new column was \n",
    "added to the file before it was uploaded, assigning each ecosystem a one-word \n",
    "'Short_Name' that will be used to identify the ecosystem in this workflow for \n",
    "identification and filenaming purposes (ex. 'dome' for 'South Florida Cypress \n",
    "Dome', 'dune' for 'Southwest Florida Dune and Coastal Grassland', and 'mesic' \n",
    "for 'Crowley's Ridge Mesic Loess Slope Forest').   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118406c0",
   "metadata": {},
   "source": [
    "#### *IN THE FUTURE -* \n",
    "\n",
    "Currently our code will work with the ecosystem raster and hex files that Lana \n",
    "created in ArcGIS using the ArcMarxan plugin.  Ultimately we hope to work directly\n",
    "with the full Landfire EVT 2020 raster, but the file is proving too large to \n",
    "effectively manage with our personal laptops. A solution may be found using the \n",
    "2016 Landfire data which has an available API (the 2020 data is scheduled to be \n",
    "published to the API later this year). Other alternative solutions for working \n",
    "with large raster files might be found using Dask, or possibly Amazon Web Services \n",
    "to access additional processing capabilities.\n",
    "\n",
    "If/When our code can generate shp and raster files for individual ecosystems, the \n",
    "'from_LF_EVT_2020_README.csv' file saved to our GitHub assets/data directory will \n",
    "need to be updated to the full version of the LF_2020_EVT_README file, after first \n",
    "assigning 'Short_Name' values to all 856 ecosystems in the file. Once that occurs, \n",
    "we could ask for user input to get entries matching the 'Short_Name' values in \n",
    "order to select specific ecosystems from the full Landfire EVT 2020 data. That user \n",
    "input would be assigned to the list variable 'short_name_filter'. \n",
    "\n",
    "If short-naming all 856 ecosystems proves to be cumbersome, the user could be \n",
    "prompted to provide the 'Value' values from the LF_EVT_2020_README (ex. 7447 for \n",
    "South Florida Cypress Dome), and then be prompted to provide the one word \n",
    "'Short_Name' for each selected value. The 'Value' entry would filter the df, and the \n",
    "'Short_Name' entry would be added to the resulting dataframe as a new column.\n",
    "\n",
    "If a user decided not to use the LF_2020_EVT_README file as their table of \n",
    "information, any .csv with the following minimum requirements could be used.\n",
    "\n",
    "Minimum required columns:\n",
    "* 'Short_Name' (the one word short name assigned to the ecosystem)\n",
    "* 'RLE_FINAL' (status in IUCN Red List of Ecosystems, using 2 letter abbreviated \n",
    "   format (ex. 'CR' for Critical, 'EN' for Endangered and 'VU' for Vulnerable)\n",
    "* 'US_km2' (total extent of ecosystem in km2)\n",
    "\n",
    "This existing code could be reused if the user were prompted for a url where \n",
    "they have their table stored\n",
    "\n",
    "Otherwise\n",
    "1. Prompt user to save their table with the minimum required columns to \n",
    "'earth-analytics/data/kba_thresh_sa' as 'ecosystem_info.csv'  \n",
    "2. Check for 'ecosystem_info.csv' in 'earth-analytics/data/kba_thresh_sa'  \n",
    "3. If found, load to dataframe  \n",
    "   If not found, prompt user to \"Save ecosystm_info.csv' to \n",
    "'earth-analytics /data/kba_thresh_sa' directory, then rerun notebook\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd802b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the csv file stored on GitHub repository \n",
    "# (contains info on selected ecosystems taken from LF_EVT_2020_README \n",
    "# file, with an added 'Short_Name' field that is used as index)\n",
    "\n",
    "# Provide the URL (using raw content at GitHub)\n",
    "ecoinfo_url = (\"https://raw.githubusercontent.com/csandberg303/\"\n",
    "               \"kba-threshold-sensitivity-analysis/main/assets/data/\"\n",
    "               \"from_LF_EVT_2020_README.csv\")\n",
    "\n",
    "# Create local cache overide variable\n",
    "cache_override = True or CACHE_OVERRIDE\n",
    "\n",
    "# Provide the path to local directory\n",
    "ecoinfo_path = os.path.normpath(\n",
    "    os.path.join(data_path, 'from_LF_EVT_2020_README.csv'))\n",
    "\n",
    "# Create dataframe from information at provided URL\n",
    "ecoinfo_df = pd.read_csv(ecoinfo_url).set_index('Short_Name')\n",
    "\n",
    "# Check for csv in local directory and create from df if needed\n",
    "if not os.path.exists(ecoinfo_path) or cache_override:\n",
    "\n",
    "    # Read csv at URL into pandas dataframe, using 'Short_Name' col as index\n",
    "    ecoinfo_df.to_csv(ecoinfo_path)\n",
    "    \n",
    "ecoinfo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b338261",
   "metadata": {},
   "source": [
    "##### Two columns are added to the 'ecoinfo_df' dataframe\n",
    "These will generate the appropriate KBA Threshold Value for each ecosystem, which \n",
    "is 5% or 10%, depending on its IUCN RLE Status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d67038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column 'Type' to assign a number of 1 or 2 based upon the value in text \n",
    "# column 'RLE_FINAL' (Type = 1 if 'CR', 'CR (CR-EN)', 'EN (CR-EN) or 'EN'; \n",
    "# Type = 2 if 'VU')\n",
    "\n",
    "# create a list of conditions\n",
    "type_conditions = [(ecoinfo_df['RLE_FINAL'] == 'CR'), \n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'CR (EN-CR)'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'EN'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'EN (EN-CR)'),\n",
    "                  (ecoinfo_df['RLE_FINAL'] == 'VU')]\n",
    "\n",
    "# create a list of the values to assign for each condition\n",
    "type_values = [1, 1, 1, 1, 2]\n",
    "\n",
    "# create new column using np.select to assign values using lists as arguments\n",
    "ecoinfo_df['Type'] = np.select(type_conditions, type_values)\n",
    "\n",
    "# 2nd column - Add column 'Current_IUCN_TH'. Uses np.select to assign a \n",
    "# threshold percentage, based upon the column 'Type' (5% if 1, 10% if 2)\n",
    "\n",
    "# create a list of conditions\n",
    "current_threshold_conditions = [(ecoinfo_df['Type'] == 1), \n",
    "                               (ecoinfo_df['Type'] == 2)]\n",
    "\n",
    "# create a list of the values to assign for each condition\n",
    "current_threshold_values = [.05, .10]\n",
    "\n",
    "# create new column using np.select to assign values using lists as arguments\n",
    "ecoinfo_df['Current_IUCN_TH'] = np.select(\n",
    "    current_threshold_conditions, current_threshold_values)\n",
    "\n",
    "ecoinfo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91130827",
   "metadata": {},
   "source": [
    "##### The two cells below provide an opportunity for the user to edit variables to control the Marxan analysis run\n",
    "\n",
    "The first of these cells set the values of two list variables.\n",
    "\n",
    "The 'test_threshold' list provides the basis for the sensitivity analysis of the IUCN KBA Thresholds.  The 'test_threshold' value of 1.0  will prompt Marxan to make selections based on the current IUCN Threshold size.  A 'test_threshold' value of 0.50 will mean Marxan will look to make cell selections at 50% of the Current IUCN Threshold size.  Our workflow is currently testing the IUCN Thresholds at four levels - 1.00, 0.75, 0.50 and 0.25.\n",
    "\n",
    "Currently the 'eco_list' variable has been hard-coded to show the three ecosystems in our initial workflow test ('dome', 'dune' and 'mesic').  \n",
    "\n",
    "The 'ecoinfo_df' is then filtered by the values in the 'eco_list'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58116e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE LISTS THAT WILL BE USED LATER IN ITERATION LOOPS\n",
    "\n",
    "# Create list of threshold values to test\n",
    "test_threshold = [1.0, 0.75, 0.50, 0.25] \n",
    "\n",
    "# Define list variable 'eco_list' to show the 'Short_Name' values of the \n",
    "# the ecosystems to be analyzed.  \n",
    "# (NOTE: THESE INDIVIDUAL LINES CAN BE COMMENTED OUT, TO DESELECT THEM FROM \n",
    "# THE CURRENT MARXAN ANALYSIS RUN)\n",
    "eco_list = [\n",
    "    'dome',\n",
    "    'dune',\n",
    "    'mesic'\n",
    "]\n",
    "eco_list.sort()\n",
    "\n",
    "# use 'eco_list' to create a new df with only matching records\n",
    "eco_subset_df = ecoinfo_df.filter(items = eco_list, axis=0)\n",
    "\n",
    "eco_subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d356b18",
   "metadata": {},
   "source": [
    "##### The second variable setting cell is where individual variable values are set\n",
    "\n",
    "For more information on 'prop', 'spf', 'numreps', 'numitns', 'blm' and 'runmode', \n",
    "please refer to the User Manuals and Best Practices documentation available on the Marxan \n",
    "website (https://marxansolutions.org/software/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93362939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES TO BE USED IN MARXAN RUN\n",
    "\n",
    "# provide a testrun_basename (this value can be whatever the user deems \n",
    "# important (but should be kept as short as possible and include no spaces);  \n",
    "# it will be appended to a timestamp when generating a directory name to \n",
    "# ensure each run will have a unique directory name.\n",
    "testrun_basename = 'testrun'\n",
    "\n",
    "# ESPG value to set as CRS for raster and shapefile\n",
    "espg = '5070'\n",
    "\n",
    "# # Set 'prop' to show the desired final proportion of ecosystem extent that \n",
    "# should be selected from Marxan (must be between 0 and 1, currently the value\n",
    "# is set at 0.3 for 30% of total extent) \n",
    "prop = 0.3\n",
    "\n",
    "# Species Penalty Factor \n",
    "spf = 1\n",
    "\n",
    "# Number of repeat runs (or solutions) - default value = 100\n",
    "numreps = 100\n",
    "\n",
    "# Number of iterations for annealing; default value = 1000000\n",
    "# (more iterations require longer processing times)\n",
    "# NOTE: RUNMODE 1 & 3 did not complete successfully with numitins=10 or 1000)\n",
    "numitns = 10000\n",
    "\n",
    "# Boundary Length Modifier (default value from qmarxan code = 1)\n",
    "blm = 1\n",
    "    \n",
    "# Runmode (determines annealing/heuristic properties to be used in Marxan run)\n",
    "runmode = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d6d4",
   "metadata": {},
   "source": [
    "With the variables set, the main work of the workflow can begin.  This will be done by setting up a series of iterative loops. The highest level loop will be for three ecosystems, followed by loops set for each the four threshold test values. 3 ecosystems x 4 test levels result in 12 'ecotest' loops.  \n",
    "\n",
    "Within each of the 12 ecotest loops, there will need to be a series of internal loops to make multiple selections for a single ecotest. Each of these internal loops represents a single KBA selection. As such, each internal loop will require it's own set up of directories and input files, all in the required Marxan format.\n",
    "\n",
    "The number of internal loops will be determined by three factors -\n",
    "* the test level value from the 'test_threshold' list\n",
    "* the size of the overall conservation target (expressed as a proportion of the total ecosystem extent)\n",
    "* the current IUCN threshold of either 5% or 10%, which is determined by the ecosystem's 'Red List of Ecosystems' status  (seen in the 'RLE_FINAL' column of the LF_EVT_2020_README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee488e",
   "metadata": {},
   "source": [
    "##### Setting the internal loop count\n",
    "The table below shows how the loop counts are set for each ecotest.  The Number of Loops required will be equal to the Number of KBAs needed to reach the Conservation Target.  \n",
    "\n",
    "For instance, the Dome ecosystem is listed as 'Vulnerable' so the Current IUCN KBA Threshold, or minimum size required for a KBA designation, would be 10% of the overall ecosystem extent.  The test at 1.00 of the current threshold would mean three KBAs will be needed to reach the overall conservation target of 30% (3 KBAs, each @ 10% of the total ecosystem extent would equal 30% of the total ecosystem extent when combined).  The loop count for this test will be 3.\n",
    "\n",
    "Another example would be the dune ecosystem at the 0.25 test level.  The dune ecoystem is listed as Critical, so its current IUCN KBA Threshold level is 5%.  Down at the 0.25 test level, the Target2 size when the IUCN TH is 5% is just 1.25% of the total extent. This means that 24 KBAs would be need to be identified to reach the 30% Conservation Target (24 KBAs @ 1.25% each = 30% total). This means that 24 loops will be needed for that test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f8460",
   "metadata": {},
   "source": [
    "|                      \t **RLE_Status** \t|\t **IUCN TH** \t|\t **Test** \t|\t **Target2 (IUCN TH x Test)** \t|\t **Conservation Target** \t|\t **Num of Loops** \t|\n",
    "|\t :--- \t|\t ----------: \t|\t -------: \t|\t ---------------------------: \t|\t ----------------------: \t|\t ----------------: \t|\n",
    "|                 \t **Vulnerable (VU)** \t|        \t 10% \t|    \t 1.00 \t|                       \t 10.0% \t|                    \t 30% \t|                \t 3 \t|\n",
    "|                                    \t  \t|        \t 10% \t|    \t 0.75 \t|                        \t 7.5% \t|                    \t 30% \t|                \t 6 \t|\n",
    "|                                    \t  \t|        \t 10% \t|    \t 0.50 \t|                        \t 5.0% \t|                    \t 30% \t|                \t 9 \t|\n",
    "|                                    \t  \t|        \t 10% \t|    \t 0.25 \t|                        \t 2.5% \t|                    \t 30% \t|               \t 12 \t|\n",
    "|                                    \t  \t|           \t  \t|        \t  \t|                            \t  \t|                       \t  \t|      \t **TOTAL: 30** \t|\n",
    "|\t **Critical (CR) or Endangered (EN)** \t|         \t 5% \t|    \t 1.00 \t|                        \t 5.0% \t|                    \t 30% \t|                \t 6 \t|\n",
    "|                                    \t  \t|         \t 5% \t|    \t 0.75 \t|                       \t 3.75% \t|                    \t 30% \t|               \t 12 \t|\n",
    "|                                    \t  \t|         \t 5% \t|    \t 0.50 \t|                        \t 2.5% \t|                    \t 30% \t|               \t 18 \t|\n",
    "|                                    \t  \t|         \t 5% \t|    \t 0.25 \t|                       \t 1.25% \t|                    \t 30% \t|               \t 24 \t|\n",
    "|                                    \t  \t|           \t  \t|        \t  \t|                            \t  \t|                       \t  \t|    \t **TOTAL: 60** \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9de68",
   "metadata": {},
   "source": [
    "Our list of three ecosystems to be analyzed includes one 'Vulnerable' (dome) and two 'Critical/Endangered'(dune and mesic).  This means that each full analysis run wil require 150 Marxan directories to be set up, populated and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b275729",
   "metadata": {},
   "source": [
    "##### Loop through the eco_list, create directories and input files needed by Marxan.\n",
    "\n",
    "Each time the code below runs, a new timestamped diretory is created. Inside will\n",
    "be subdirectories created from the 'Short_Name' value of the selected ecosystems \n",
    "seen in the 'eco_subset' variable.\n",
    "\n",
    "Each of these ecosystem subdirectories will have the following named \n",
    "subdirectories -\n",
    "* input - where files needed by marxan analysis are stored (bound.dat, pu.dat, \n",
    "puvsp.dat, spec.dat)\n",
    "* output - where files generated by marxan analysis are stored\n",
    "* pu - pu and report seen in qmarxan setup (purpose tbd)\n",
    "* report - pu and report seen in qmarxan setup (purpose tbd)\n",
    "* source data - where the rasters and PU hex_shp files are moved to, after they\n",
    "are copied from the 'r_tif' and 'hex_shp' folders\n",
    "\n",
    "A fifth input file 'input.dat' is created and placed in the main ecosystem \n",
    "directory.\n",
    "\n",
    "Of the five required input files, three are created in the workflow (input.dat, \n",
    "pu.dat and spec.dat).  The remaining two (bound.data and puvsp.dat) have been \n",
    "created in ArcGIS using the ArcMarxan toolbox plug in (a parallel Qmarxan plugin \n",
    "is avaiable for QGIS). In these two cases, the worflow uses a formula\n",
    "to copy those files from their saved location in the GitHub repository and save\n",
    "them to the appropriate input folder.\n",
    "\n",
    "#### *IN THE FUTURE -* \n",
    "* Our project sponsor has said that the required set of Marxan input files are \n",
    "commonly prepared using GIS tools, due to the complexity of the spatial \n",
    "calculations involved.  If that is the practice we will continue, but the process\n",
    "would be a pinch point without automation.  It may prove useful to learn how to \n",
    "work with python console window in QGIS, to see if the QMarxan plugin could be \n",
    "integrated into the workflow. Creating the input files programatically rather \n",
    "manually within GIS may allow for easier manipulation of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee9f6b",
   "metadata": {},
   "source": [
    "##### Call Marxan Executable within each loop.  \n",
    "As the workflow iterates through the loops, information in output to the\n",
    "screen in order to ensure the loops are progressing logically and is using \n",
    "accurate input parameters.\n",
    "\n",
    "As Marxan completes a run, output files are generated and saved.  Some of this\n",
    "output is needed as input for the next run, so a pause is generated to allow the \n",
    "file writing process to complete before the workflow attempts to access that \n",
    "information.  Two options are provided for managing the pause, either by manually \n",
    "clicking the 'Enter' button after seeing that each of the 150 Marxan pop-up \n",
    "windows has completed execution, or by setting a sleep timer to pause for \n",
    "slightly longer than the Marxan execution will take.  Setting the timer is \n",
    "less labor-intensive, but runs the risk of wasting time if set too long, or \n",
    "generating errors if the the files if the timer is set to be too short. \n",
    "\n",
    "Ideally, the workflow will find the 'best_run' output file generated by Marxan, \n",
    "in order to get the list of cells that were selected in that run.  That list of \n",
    "cells is used to update two files; one is used as the new input file of the next \n",
    "run and the other will keep track of all selected cells in the ecotest so that\n",
    "each selection is able to be measured individually.  Doing this will mean that \n",
    "a cell can only be selected one time for each ecotest, and the remaining loops \n",
    "will be forced to find solutions using other cells that are still available.  \n",
    "As a result, each ecotest will show multiple selections that each meet a minimum \n",
    "size requirement, and when added together would reach the overall Conservation \n",
    "Target set in the 'prop' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8573264",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ********* NEW TEST FOR WHILE LOOP *********\n",
    "# USE THIS CELL FOR MARXAN v4.06 AND MARXAN v2.43 (CURRENTLY USING 2.43)\n",
    "\n",
    "# RUN THIS CELL TO BEGIN AUTOMATED WORKFLOW \n",
    "# (1ST CELL OF TWO - BEGIN MARXAN ANALYSIS)\n",
    "\n",
    "# checks to see if a directory based upon provided 'testrun_basename' has \n",
    "# already been made. If so, a number will be added to the end \n",
    "# 'testrun_basename' before creating new directory (so that each named \n",
    "# directory will maintain a unique ID beyond the timestamp).\n",
    "testrun_basename_count = 0\n",
    "testrun_basename_glob = glob(os.path.join(\n",
    "    data_path, '*' + testrun_basename + '*'))\n",
    "testrun_basename_count = len(testrun_basename_glob)\n",
    "if testrun_basename_count>0:\n",
    "    testrun_basename = (testrun_basename + \n",
    "    str(f\"{(testrun_basename_count + 1):02d}\"))\n",
    "print('testrun_basename_count = ' + str(testrun_basename_count))\n",
    "print('testrun_basename: ' + testrun_basename)\n",
    "\n",
    "# set new directory name, based upon timestamp and provided 'testrun_basename'\n",
    "new_dir = os.path.normpath(\n",
    "    os.path.join(data_path, datetime.datetime.now().strftime('%Y%m%d_%H%M%S') \n",
    "                 + '_' + testrun_basename))\n",
    "os.makedirs(new_dir)\n",
    "print(new_dir)\n",
    "\n",
    "# Set 'heurtype' - Determined by runmode entry in input.dat \n",
    "# if RUNMODE = 3 then use heurtype = 1 (greedy), else -1 (not used)\n",
    "# (NOTE: this variable is used for RUNMODE 3 only, and currently this\n",
    "# multiloop workflow is using RUNMODE 1. Keeping it in notebook in case that \n",
    "# may ever change)\n",
    "if runmode == 3:\n",
    "    heurtype = 1\n",
    "else:\n",
    "    heurtype = -1\n",
    "print('runmode: ' + str(runmode) +'\\nheurtype: ' + str(heurtype))\n",
    "print('threshold tests: ' + str(test_threshold))\n",
    "\n",
    "selection_loop_ls = []\n",
    "test_loop_ls = []\n",
    "eco_loop_ls = []\n",
    "\n",
    "### 1ST LOOP BEGINS HERE \n",
    "\n",
    "# LOOP THROUGH ECOSYSTEMS (in 'ecolist') \n",
    "for eco in eco_list:\n",
    "    print('\\nbegin ecoloop: ' + eco)\n",
    "    os.chdir(new_dir)\n",
    "    # create directory for each ecosystem selected for analysis\n",
    "    os.makedirs('eco_' + eco)\n",
    "    os.chdir('eco_' + eco)\n",
    "    eco_data_path = os.path.normpath(os.path.join(new_dir, 'eco_' + eco))\n",
    "    # create 'source_data' directory to store ArcGIS shp and tif files\n",
    "    # THESE WILL BE USED BY EACH NEW TEST DIRECTORY CREATED IN IN THE 'eco' \n",
    "    # DIRECTORY\n",
    "    os.makedirs('source_data')\n",
    "    os.chdir('source_data')\n",
    "    source_data_path = (new_dir, 'eco_' + eco, 'source_data')\n",
    "    # copy source files that were stored locally to the 'hex_shp' and 'r_tif' \n",
    "    # directories after running 1st notebook. Our workflow is currently  \n",
    "    # using the files Lana created manually using ArcGIS\n",
    "    ks.get_source_files_targetloops(os.path.join(data_path, \"hex_shp\"), eco)\n",
    "    ks.get_source_files_targetloops(os.path.join(data_path, \"r_tif\"), eco)\n",
    "     \n",
    "    # create 'orig_input_files' directory to store the 5 .dat files) for the \n",
    "    # ecosystem so that each Marxan analysis loop will pick them up from this \n",
    "    # location (since much info for the analysis runs will remain constant as \n",
    "    # the KBA threshold size is tested) \n",
    "    os.chdir(eco_data_path)\n",
    "    os.makedirs('orig_input_files')\n",
    "    os.chdir('orig_input_files')\n",
    "    orig_input_data_path = os.path.normpath(os.path.join(data_path, \n",
    "                                                         new_dir, \n",
    "                                                         'eco_' + eco, \n",
    "                                                         'orig_input_files'))\n",
    "    \n",
    "    # CREATE INPUT FILES THAT WILL REMAIN CONSTANT DESPITE TEST LEVELS \n",
    "    # (pu.dat, puvsp.dat, bound.dat).  \n",
    "    \n",
    "    # CREATE INITIAL PU.DAT FROM ORIGINAL FORMULA \n",
    "    # Provides a record of each planning unit hex cell in the .shp file,  \n",
    "    # using a default uniform cost of '1', and  a status of '0' which \n",
    "    # indicates that unit is avaialable to Marxan for selection. As the loops \n",
    "    # continue until set proportion target of 30% overall extent is reached, \n",
    "    # this pu.dat file will be updated so that selected cells will show a \n",
    "    # status value of '3' for unavailable/locked-out.\n",
    "    ks.create_pu_dat_targetloops(eco, \n",
    "                                 eco_data_path)  \n",
    "    orig_pu_dat_path = os.path.normpath(os.path.join(\n",
    "        orig_input_data_path, 'pu.dat'))\n",
    "    pu_dat = pd.read_csv(orig_pu_dat_path)\n",
    "    \n",
    "    # THEN CREATE 2 ADDITIONAL DATAFRAMES BASED OFF THE INITIAL 'pu_dat' FILE\n",
    "\n",
    "    # 1) 'pu_selected' \n",
    "    # This df will be used to keep an overall record of which cell was \n",
    "    # selected in each loop, so that each loop's selection can be seen and \n",
    "    # measured independently.\n",
    "    pu_selected = pu_dat.set_index('id')\n",
    "    # Set initial value of 'selection' column to 'not selected'\n",
    "    pu_selected['selection'] = 'not selected'\n",
    "    \n",
    "    \n",
    "    # 2) 'updated_pu_dat'\n",
    "    # This df will be used to track selected cells as loops progress, so that \n",
    "    # those cells will be locked out of selection in future loops.\n",
    "    # (THIS BECOMES THE NEW 'pu.dat' INPUT FILE IN FUTURE LOOPS)\n",
    "    updated_pu_dat = pu_dat.set_index('id')\n",
    "\n",
    "    \n",
    "     \n",
    "    # USE 'get_marxan_input_files' FUNCTION TO COPY IN ANY REMAINING .DAT \n",
    "    # FILES NEEDED THAT ARE CREATED IN ArcGIS/QGIS RATHER THAN PYTHON.\n",
    "    # This formula currently is used for 'bound.dat' and 'puvsp.dat'.\n",
    "    # Formula will copy files that have been created using ArcMarxan tool \n",
    "    # in ArcGIS then saved to the repository.\n",
    "    ks.get_marxan_input_files_targetloops(eco, \n",
    "                                          ['bound.dat', \n",
    "    #                                     \"pu.dat\", \n",
    "                                           'puvsp.dat', \n",
    "    #                                     \"spec.dat\"\n",
    "                                          ])\n",
    "    bound_dat_path = os.path.normpath(os.path.join(\n",
    "        orig_input_data_path, 'bound.dat'))\n",
    "    bound_dat = pd.read_csv(bound_dat_path)\n",
    "    puvsp_dat_path = os.path.normpath(os.path.join(\n",
    "        orig_input_data_path, 'puvsp.dat'))\n",
    "    puvsp_dat = pd.read_csv(puvsp_dat_path)\n",
    "    \n",
    "    # THE MAIN LOOP WILL BEGIN HERE, STARTING IN THE 'eco' DIRECTORY\n",
    "    os.chdir(eco_data_path)\n",
    "    \n",
    "    # create empty list variable that will be used to collect summary info \n",
    "    # from loops, and set initial loop count to one\n",
    "    select_summary_ls = []\n",
    "\n",
    "    # LOOP THROUGH EACH KBA THRESHOLD SIZE TEST \n",
    "    # (these are the values set earlier in the 'test_threshold' list)\n",
    "    for test in test_threshold:\n",
    "        print('\\n  Begin test loop: ' + eco + ' test ' + str(test))\n",
    "       \n",
    "        # get Current IUCN KBA Threshold for ecosystem (10% VU, 5% CR or EN)\n",
    "        current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "        # get ecosystem extent in km2\n",
    "        us_km2 = eco_subset_df.at[eco,'US_km2']\n",
    "        # convert ecosystem extent in km2 to ecosystem extent in m2\n",
    "        us_m2 = us_km2 * 1000000\n",
    "        # set 'target2' variable to equal the minimum size requirement (in m2) \n",
    "        # for a KBA designation within given ecosystem, at given test level\n",
    "        # (target2 = ecosystem extent in m2 x Current_IUCN_TH x test level)\n",
    "        target2 = us_m2 * current_iucn_th * test\n",
    "        target2 = round(target2)\n",
    "\n",
    "        # set the 'target' variable used in spec.dat file to equal 'target2'\n",
    "        # (Doing this will prompt Marxan to find a selection of this size,\n",
    "        # once for each time the executable is called within a loop)\n",
    "        target = target2\n",
    "                \n",
    "        # create Scenario ID from 'eco' & 'test' variable values\n",
    "        # (used will be used as prefix in filenames, so any '.' that exist in \n",
    "        # 'test' variable will be removed) \n",
    "        scen_id = (eco + str(test).translate(\n",
    "            str.maketrans('', '', '.')) + '_run')\n",
    "               \n",
    "        print('  ' + scen_id + ': target2 (km2) = ' + str(target2/1000000))\n",
    "         \n",
    "        # CREATE INPUT FILES IN THE 'FOR TEST IN TEST THRESHOLD' LOOP WHEN \n",
    "        # THEY REQUIRE INFORMATION AT THE TEST LEVEL (input.dat, spec.dat)\n",
    "             \n",
    "        # CREATE 'input.dat' FILE USING FORMULA ADAPTED FROM 'qmarxan_toolbox' \n",
    "        # (including the 'formatAsME' format as Marxan Exponent function)\n",
    "        # Some input parameters are provided to the formula, to replace the \n",
    "        # default values that were provided in the qmarxan code. \n",
    "        ks.create_input_dat(orig_input_data_path, \n",
    "                            blm, \n",
    "                            numreps, \n",
    "                            numitns, \n",
    "                            runmode, \n",
    "                            heurtype, \n",
    "                            scen_id)\n",
    "        input_dat_path = os.path.normpath(os.path.join(\n",
    "            orig_input_data_path, \"input.dat\"))\n",
    "        input_dat = pd.read_csv(input_dat_path)\n",
    "        \n",
    "        # CREATE THE 'spec.dat' FILE FROM v4 FORMULA (includes 'target' only)\n",
    "        os.chdir(orig_input_data_path)\n",
    "        ks.create_spec_dat_v4_targetloops(eco_subset_df, eco, target, spf)\n",
    "        spec_dat_path = os.path.normpath(os.path.join(\n",
    "            orig_input_data_path, 'spec.dat'))\n",
    "\n",
    "        # Print initial info statement for test loop and begin creating the \n",
    "        # needed directories\n",
    "        os.chdir(eco_data_path)\n",
    "        os.makedirs(scen_id) \n",
    "        ecotest_data_path = os.path.normpath(os.path.join(data_path, new_dir, \n",
    "                                                          'eco_' + eco, \n",
    "                                                          scen_id))\n",
    "        # create path for 'pu_selected' file in 'scen_id' directory\n",
    "        # (this file will store information from the 'pu_selected' df created\n",
    "        # earlier, which allows for an overall record to be kept showing which \n",
    "        # cell was selected in each loop of a test, so that each selection can \n",
    "        # be seen and measured independently)\n",
    "        pu_selected_path = os.path.normpath(os.path.join(\n",
    "            new_dir, \n",
    "            'eco_' + eco,\n",
    "            scen_id,\n",
    "            scen_id + '_pu_selected.csv'))\n",
    "        \n",
    "        # create path for 'updated_pu_dat' file in 'scen_id' directory\n",
    "        updated_pu_dat_path = os.path.normpath(os.path.join(\n",
    "            new_dir, \n",
    "            'eco_' + eco, \n",
    "            scen_id, \n",
    "            scen_id + '_updated_pu.dat'))\n",
    "\n",
    "        # SET 'end_count' VALUE TO END MULTILOOP\n",
    "        # Based upon given 'prop' value of 30% and 'Current_IUCN_TH' \n",
    "        # 1st ex: if prop = 30% & eco is VU(KBA 10%); then 3 x 10% KBA = 30% \n",
    "        # end_count = 3 (@ 1.0 test), 6 @ 0.50 test and 12 @ 0.25 test\n",
    "        # 2nd ex: if prop = 30% & eco is CR/EN(KBA 5%); then 6 x 5% KBA = 30% \n",
    "        # end_count = 6 (@ 1.0 test), 12 @ 0.50 test and 24 @ 0.25 test\n",
    "        end_count = round(prop/(current_iucn_th * test))\n",
    "        print('  ' + scen_id + ': end count = ' + str(end_count) + '\\n')\n",
    "        \n",
    "        # initialize 'loop_count' variable to 1\n",
    "        loop_count = 1\n",
    "        \n",
    "        # BEGIN MULTILOOP FOR EACH TEST IN EACH ECOSYSTEM DIRECTORY        \n",
    "        while loop_count <= end_count:\n",
    "            # create string of loop count, to allow for alphabetization of \n",
    "            # the resulting filenames\n",
    "            loop_count_str = 'loop_' + str(f\"{loop_count:02d}\")\n",
    "            print('    ' + scen_id + \" \" + loop_count_str + ' | end_count = ' \n",
    "                  + str(end_count))\n",
    "            \n",
    "            # begin loop in the 'ecotest' directory (ex. 'mesic025')\n",
    "            os.chdir(ecotest_data_path)\n",
    "            # create directory for loop, to store Marxan input/output files\n",
    "            os.makedirs(loop_count_str)\n",
    "            loop_count_path = os.path.normcase(os.path.join(\n",
    "                ecotest_data_path, loop_count_str))\n",
    "            os.chdir(loop_count_path)\n",
    "            \n",
    "            # COPY IN THE INPUT FILE FROM 'orig_input_files' DIRECTORY\n",
    "            shutil.copy(input_dat_path, os.getcwd())\n",
    "\n",
    "            # CREATE INPUT DIRECTORY\n",
    "            # which is where the four remaining .dat files will be stored\n",
    "            os.makedirs('input')\n",
    "            eco_input_data_path = os.path.normpath(os.path.join(\n",
    "                ecotest_data_path, loop_count_str, 'input'))\n",
    "            os.chdir(eco_input_data_path)\n",
    "            \n",
    "            # COPY IN THE 3 DAT FILES THAT WILL REMAIN UNCHANGED AS LOOPCOUNT\n",
    "            # PROGRESSES (bound.dat, puvsp.dat and spec.dat) \n",
    "            unchanged_dat_files = (bound_dat_path, \n",
    "                                   puvsp_dat_path, \n",
    "                                   spec_dat_path)\n",
    "            for file in unchanged_dat_files:\n",
    "                shutil.copy(file, os.getcwd())\n",
    "        \n",
    "            # GET APPROPRIATE 'pu.dat' FILE FOR LOOP_COUNT\n",
    "            # This is where the 'loop_count' variable determines if the  \n",
    "            # original 'pu_dat' file should be used (if loop 1), or if the \n",
    "            # 'updated_pu_dat' file generated from the previous loops should  \n",
    "            # be used (for loops 2-End) \n",
    "            if loop_count == 1:\n",
    "                shutil.copy(orig_pu_dat_path, os.getcwd())\n",
    "                pu_dat = pd.read_csv(orig_pu_dat_path)\n",
    "            else:\n",
    "                shutil.copy(updated_pu_dat_path, os.getcwd())\n",
    "                os.rename(scen_id + '_updated_pu.dat','pu.dat')\n",
    "            \n",
    "            # create remaining directories\n",
    "            os.chdir(loop_count_path)\n",
    "            os.makedirs('output')\n",
    "            os.makedirs('report')\n",
    "            os.makedirs('pu')      \n",
    "\n",
    "            # BEGIN MARXAN ANALYSIS RUN\n",
    "            print('    ' + scen_id + \" \" + loop_count_str + \n",
    "                  ': MARXAN ANALYSIS INITIATED')   \n",
    "            # call on marxan executable (currently using v2.43)\n",
    "            os.startfile(marxan_243_path)\n",
    "            \n",
    "            # DEFINE A PAUSE FOR MARXAN EXECUTION, USING ONE OF TWO METHODS\n",
    "            # This is needed to allow Marxan time to finish writing \n",
    "            # output files before the workflow tries to locate them            \n",
    "            # NOTE: ONE OPTION MUST BE COMMENTED OUT BEFORE RUNNING THE CELL\n",
    "            # SO THAT IT WILL BE IGNORED DURING WORKFLOW \n",
    "            # (To do this, highlight rows of the option that should not be \n",
    "            # used, and hit 'CTRL + /' to toggle that selection to show green \n",
    "            # text lines beginning with #.  Pause Option 1 is currently \n",
    "            # inactive, and the the sleep timer has been set for 30 seconds)\n",
    "\n",
    "#             # PAUSE OPTION 1: Hit Enter to Continue \n",
    "#             # Wait for Marxan pop-up execution to complete, then press 'Enter'\n",
    "#             # at prompt in screen output window after 'The End' is seen\n",
    "#             # (overall quickest, but requires attention)\n",
    "#             def pause():\n",
    "#                 programPause = input(\"Press the <ENTER> key to continue...\")\n",
    "#             pause()\n",
    "#             print('Wait to see 'The End' at bottom of Marxan execution '\n",
    "#                  'pop-up before pressing Enter')\n",
    "\n",
    "            # PAUSE OPTION 2: Set sleep timer length\n",
    "            # Define a sleep timer so that Python will simply count down that \n",
    "            # number of seconds before moving on. Need to ensure that the \n",
    "            # sleep time set > filewriting/execution, or errors in reading \n",
    "            # output files will occur\n",
    "            # (automates the workflow, but takes longer time overall)\n",
    "            # set 'sleeptime' variable for length of pause in seconds\n",
    "            # (for my system, I use 20 seconds when numitns = 1000000 and \n",
    "            # 2 seconds when numitns = 10000)\n",
    "            sleeptime = 5\n",
    "            print('    time.sleep(' + str(sleeptime) + ') applied to pause '\n",
    "                  'workflow execution for ' + str(sleeptime) + ' seconds '\n",
    "                  'while Marxan output files are written')\n",
    "            time.sleep(sleeptime) \n",
    "          \n",
    "          \n",
    "            # WHEN MARXAN COMPLETES, GET BEST RUN SOLUTION AND USE IT'S \n",
    "            # 'SOLUTION' COLUMN VALUES TO UPDATE THE 'pu_selected' AND \n",
    "            # 'updated_pu_dat' dataframes\n",
    "\n",
    "            # First check for output files, to see if the run had errors\n",
    "            # open '_best' file created by Marxan and saved to 'output' dir\n",
    "            globfile_best = glob(os.path.normpath(os.path.join(\n",
    "                ecotest_data_path, loop_count_str, 'output', '*_best.csv')))            \n",
    "            # if no file is found, print error message to screen\n",
    "            if globfile_best == []:\n",
    "                output = print (scen_id + \": ERROR: 'pu_selected' file not \"\n",
    "                                'found - check output/log. \\nWill need to '\n",
    "                                'resolve error and rerun Marxan if final '\n",
    "                                \"output files haven't completed successfully\")  \n",
    "            else:\n",
    "                # Create list of selected cells from 'best_run' output file\n",
    "                best_run_file = pd.read_csv(globfile_best[0])\n",
    "                selected_df = best_run_file[best_run_file['SOLUTION'] == 1]\n",
    "                selected_cells = selected_df['PUID'].tolist()\n",
    "                print('    ', selected_cells)\n",
    "                for puid in selected_cells:\n",
    "                    # Update the status of those cells in 'pu_selected' df's \n",
    "                    # 'selection' column, to show in which run they were \n",
    "                    # selected\n",
    "                    pu_selected.at[puid, 'selection'] = ('Select_' + \n",
    "                                                         loop_count_str)\n",
    "                    # Update the status of those cells in 'updated_pu_dat' \n",
    "                    # df's'status' column, from '0'-available to \n",
    "                    # '3'-unavailable/locked-out\n",
    "                    updated_pu_dat.at[puid,'status']=3   \n",
    "                    \n",
    "                # save updated 'updated_pu_dat' file for next loop\n",
    "                updated_pu_dat.to_csv(updated_pu_dat_path)\n",
    "        \n",
    "            # add +1 to 'loop_count', and continue 'while' loop\n",
    "            loop_count = loop_count+1\n",
    "            \n",
    "        \n",
    "            print('    ' + scen_id + loop_count_str + '  ' + \"start of \" + \n",
    "                  scen_id + loop_count_str + \" end actions\")\n",
    "            # add additional summary info to 'pu_selected' df\n",
    "            # THIS SHOULD OCCUR AT END OF ECOTEST, NOT AT END OF LOOP\n",
    "            pu_selected['dir_path'] = new_dir\n",
    "            pu_selected['Short_Name'] = eco               \n",
    "            pu_selected['current_test_level'] = test\n",
    "        #         pu_selected['test_loop'] = test_loop_str\n",
    "        #                 pu_selected['lc = selection? clump from lc']\n",
    "            pu_selected['Current_IUCN_TH'] = current_iucn_th\n",
    "            pu_selected['US_km2'] = us_km2\n",
    "            pu_selected['US_m2'] = us_m2\n",
    "            pu_selected['30% of US_m2'] = us_m2*prop\n",
    "            pu_selected['KBA @ current test (m_2)'] = target2\n",
    "            pu_selected['KBA @ current test (km_2)'] = target2/1000000\n",
    "            pu_selected['BLM'] = blm\n",
    "            pu_selected['SPF'] = spf\n",
    "            \n",
    "            # Append 'pu_selected' to 'selection_loop_ls' list, so that all \n",
    "            # information from teh selection loop will be incorporated into \n",
    "            # the '_initial_loop_summary.csv'\n",
    "            selection_loop_ls.append(pu_selected)\n",
    "            print('    ' + scen_id + loop_count_str + \n",
    "                  \" selection loop info appended to to selection_loop_ls\\n\")\n",
    "        \n",
    "        # Concatenate 'selection_loop_ls and append 'sel_loop_df' to \n",
    "        # 'test_loop_ls' list, so that all information from the test loop\n",
    "        # will be incorporated into the '_initial_loop_summary.csv'\n",
    "        sel_loop_df = pd.concat(selection_loop_ls)\n",
    "        test_loop_ls.append(sel_loop_df)\n",
    "        print('' + scen_id + '  ' + \n",
    "              \"test loop info appended to to test_loop_ls\")\n",
    "                 \n",
    "\n",
    "        # Once 'end_count' is reached ('loop_count' = 'end_count'), save the \n",
    "        # 'pu_selected' df as a .csv file \n",
    "        pu_selected.to_csv(pu_selected_path) \n",
    "\n",
    "        print('' + scen_id + ' info from ' + scen_id  \n",
    "              + ' will be added to final summary\\nEnd testloop: ' \n",
    "              + scen_id + '\\n')\n",
    "\n",
    "        \n",
    "    # Concatenate 'test_loop_ls and append 'test_loop_df' to \n",
    "    # 'eco_loop_ls' list, so that all information from the eco loop\n",
    "    # will be incorporated into the '_initial_loop_summary.csv'\n",
    "    test_loop_df = pd.concat(test_loop_ls)\n",
    "    eco_loop_ls.append(test_loop_df)\n",
    "    print(eco + '  ' + \"eco loop info appended to to eco_loop_ls\\nEnd of \" + \n",
    "          eco + ' loop\\n')\n",
    "\n",
    "# Concatenate 'eco_loop_ls' and save as 'testrun_basename_df', and save  \n",
    "# information from all ecosytem, test and selection loops to \n",
    "# 'Initial Loop Summary.csv' \n",
    "testrun_basename_df = pd.concat(eco_loop_ls)\n",
    "testrun_basename_df.to_csv(os.path.normpath(os.path.join(\n",
    "    new_dir, testrun_basename + '_initial_loop_summary.csv')))\n",
    "print(testrun_basename + '  ' + 'all info s/b concat into ' + \n",
    "      testrun_basename + '_initial_loop_summary.csv')\n",
    "    \n",
    "print(scen_id + 'End of ' + testrun_basename + 'initial workflow loop\\n')\n",
    "      \n",
    "# # save info from loop stored in 'select_summary_ls' to '_final_summary.csv'\n",
    "# final_summary_df = pd.concat(select_summary_ls)\n",
    "\n",
    "# final_summary_df.to_csv(os.path.normpath(\n",
    "# os.path.join(new_dir, testrun_basename + 'final_summary.csv')), index=False)\n",
    "      \n",
    "# print(testrun_basename + \"'final_summary.csv' saved to \" + new_dir)\n",
    "        \n",
    "os.getcwd()\n",
    "print('\\nloop completed successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89454ff",
   "metadata": {},
   "source": [
    "##### inserting line to break execution betweeen workflow loops\n",
    "This will allow time to review the screen output to see if any issues would prevent \n",
    "successful execution of the second looping cell which collects summary information and \n",
    "generates plots. \n",
    "\n",
    "(To continue with execution of the final cell, click the arrow left of cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_here_for_pause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845071d",
   "metadata": {},
   "source": [
    "##### Final cell will loop through the output to generate plots and summary csvs\n",
    "A plot will be made for each of the 12 ecotest loops, showing the selections made in the \n",
    "'best_run' solution. A heatmap plot will also be generated for each ecosystem, which shows\n",
    "each hexcell's proportion of the total ecosystem extent.  These 15 plots are saved \n",
    "individually as .png files, and also combined into a single 'combined plots' pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2497b1",
   "metadata": {},
   "source": [
    "#### *IN THE FUTURE -* \n",
    "\n",
    "**Most importantly, I've noticed how the selection plots only seem to work for the \n",
    "test @ 1.00, as other test levels will show an overall selection of more than 30% \n",
    "total extent.  I trust there's something in the calculations or input files that \n",
    "is off and can be easily corrected once discovered.  I've been focusing on the \n",
    "looping logic recently and have just begun to review the results of those loops.  \n",
    "I would also like to work with the Legend of the selection plot, so that it shows \n",
    "outside the map area and can possibly include the size in (km2) of each selected \n",
    "area.**\n",
    "\n",
    "Work still needs to be done to  see if adjusting any of the input variables will\n",
    "help to achieve improved results in how Marxan makes it's selections.  As it \n",
    "currently stands, Marxan is not consistently finding cohesive clumps with each \n",
    "internal loop.  A selection may be the right size, but when the information is \n",
    "plotted it shows as multiple parts, rather than one single connected shape.  This \n",
    "is an issue since if the selection is not connected as a single unit of a minimum \n",
    "size, it can not be designated as a KBA as it is not recognized as a managable \n",
    "conservation area. If no solution for this problem can be found by adjusting the \n",
    "input parameters, perhaps the output files can be reviewed to see if they provide \n",
    "any markers or flags to indicate which selections are fully connected.  One thought\n",
    "is to look at the geometry columns of the selections, to see if they may identify\n",
    "which selections appear to be grouped in close proximity and which are spread out\n",
    "across a distance.  If such a solution can be found, then any solutions that don't \n",
    "show these markers can be filtered out.\n",
    "\n",
    "More analysis of the final summary is needed, to delve into the details and \n",
    "provide summary pivots what is most useful. For instance, the selection plots \n",
    "would benefit by showing the measured area of each selection to help validate \n",
    "whether or not the Marxan run has found a viable solution.  \n",
    "\n",
    "In recent weeks there was a change in direction with this project, abandoning the \n",
    "attempt of getting the 'target2' variable to function as it's described and instead \n",
    "trying  to replicate it's effect by using loops.  While its been very rewarding to \n",
    "begin to see multiple selections on a single plot, the original looping logic was \n",
    "altered significantly in the process which caused havoc with the csv summaries. \n",
    "Marxan provides a voluminous amount of information in its output files, beyond \n",
    "what has been collected in the loops and saved to the final summary csvs created\n",
    "in this workflow.  The challenge of how to put it to best use can be significant.  \n",
    "The Marxan output files need a closer review, and the workflow-generated .csv \n",
    "files also need to be reviewed/validated to be sure they are showing the best \n",
    "available information.  Ideally the workflow csv created in each of the looping\n",
    "cells will be combined into a single file.\n",
    "\n",
    "This is certainaly a work in progress, rich with opportuntiies for refinements \n",
    "both in the workflow itself with the interplay of QGIS and Python, the output \n",
    "summaries and plots, and also the review of the output to determine if changes to \n",
    "the input parameters could be beneficial in achieving better results. It's a bit \n",
    "unfortunate that the learning curve of this project did not dovetail neatly with \n",
    "the required timeline of the Earth Analytics Summer 2022 course. There have been \n",
    "significant gains despite the tradeoffs in the past few weeks. I will continue \n",
    "to work on this project after class ends, and am excited to see where it may go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2f77c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2ND PART OF CURRENT WORKFLOW - MARXAN 2.43 or 4.06 ONLY\n",
    "\n",
    "# THIS WILL MERGE INFORMATION FROM THE 'pu_selected' FILE CREATED IN 1ST \n",
    "# WORKFLOW TO THE HEX SHAPEFILE, IN ORDER TO BE ABLE TO SHOW WHICH HEXES WERE \n",
    "# SELECTED.  THEN THE 'puvsp.dat' INPUT FILE WILL BE MERGED TO THE SHAPEFILE, \n",
    "# TO PROVIDE THE AMOUNT OF ECOSYSTEM (in m2) CONTAINED IN EACH HEX.  THIS \n",
    "# ALLOWS THE AREA OF THE SELECTION TO BE MEASURED AND PLOTTED, TO DETERMINE \n",
    "# IF THE SOLUTION MEETS REQUIREMENTS.  SUMMARY INFORMATION FROM THE RUNS WILL \n",
    "# ALSO BE COLLECTED AND SAVED TO A .CSV FILE FOR FURTHER ANALYSIS.\n",
    "\n",
    "# Create empty lists outside the loop to store information:\n",
    "# for plot images \n",
    "plot_im_list = []\n",
    "\n",
    "# for info from shapefile, after its merged with 'puvsp.dat' and 'pu_selected'\n",
    "test_loop_ls = []\n",
    "\n",
    "\n",
    "os.chdir(new_dir)\n",
    "print('\\nnew_dir ' + new_dir)\n",
    "\n",
    "# Define 'ecotestdirs' glob list, to find all directories ending in '*_run' \n",
    "# (ex. 'dome025_run')\n",
    "ecotestdirs = sorted(glob(os.path.join(data_path, '*' + testrun_basename, '*', \n",
    "                                       '*_run')))\n",
    "print('ecotestdirs include ' + str(len(ecotestdirs)) + ' directories')\n",
    "print('ecotest_data_path ' + ecotest_data_path)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for ecotestdir in ecotestdirs:\n",
    "\n",
    "    scen_id = os.path.split(ecotestdirs[count])[1]\n",
    "    print('\\nBegin loop for scen_id: ' + scen_id)\n",
    "    os.chdir(ecotestdir)\n",
    "    print(ecotestdir)\n",
    "    get_eco = (''.join([i for i in scen_id if not i.isdigit()]))\n",
    "    eco = get_eco.replace(get_eco[(len(get_eco)-4):], '')\n",
    "    eco_data_path = os.path.normpath(os.path.join(new_dir, 'eco_' + eco))\n",
    "    ecotest_data_path = os.path.normpath(os.path.join(new_dir, 'eco_' + eco, \n",
    "                                                      scen_id))\n",
    "    \n",
    "    # create 'selected_plot' dir if it doesn't already exist\n",
    "    # (this is where the plots showing selected hexes will be stored)\n",
    "    selected_plot_dir_path = os.path.normpath(os.path.join(ecotestdir, \n",
    "                                                          'selected_plot'))\n",
    "    if os.path.isdir(selected_plot_dir_path):\n",
    "        os.chdir(selected_plot_dir_path)\n",
    "    else:\n",
    "        os.makedirs('selected_plot')\n",
    "        os.chdir('selected_plot')\n",
    "    print('cwd ' + os.getcwd())\n",
    "   \n",
    "    # try to open 'pu_selected' file created in first workflow loop cell \n",
    "    globfile_selected = glob(os.path.normpath(\n",
    "        os.path.join(ecotestdir, '*pu_selected*')))\n",
    "    print('globfile_selected contains: ' + globfile_selected[0])\n",
    "    if globfile_selected == []:\n",
    "        output = print (scen_id + \": ERROR: 'pu_selected' file not found\")\n",
    "    else:\n",
    "        # If found, merge 'pu_selected' with reprojected copy of the shp file\n",
    "        # check if reprojected shp in 'source_data' already exists; create if\n",
    "        # not found\n",
    "        shp_layer_crs_path = os.path.normpath(os.path.join(\n",
    "            new_dir, \n",
    "            'eco_' + eco, \n",
    "            'source_data', \n",
    "            eco + \"_espg_\" + espg +'.shp'))\n",
    "        print('shp_layer_crs_path: ' + shp_layer_crs_path)\n",
    "        print('eco: ' + eco)\n",
    "        if glob(os.path.normpath(os.path.join(\n",
    "            new_dir, '*', 'source_data', eco + \"_espg_\" + espg +'.shp'))):\n",
    "            print('reprojected shp file check = PASS')\n",
    "        else:\n",
    "            # open the original shp file from 'eco_data_path/source_data' \n",
    "            orig_shp_data_path = glob(os.path.join(new_dir, '*', \n",
    "                                                   \"source_data\", \n",
    "                                                   eco + '.shp'))[0]\n",
    "            print('\\n reprojecting source shapefile;\\norig_shp_data_path ' \n",
    "                  + orig_shp_data_path)\n",
    "            orig_shp_layer = gpd.read_file(orig_shp_data_path)\n",
    "            # reproject CRS of shp\n",
    "            shp_layer_crs = orig_shp_layer.to_crs(epsg=espg)\n",
    "            # create new .shp file\n",
    "            shp_layer_crs.to_file(shp_layer_crs_path, index=False)\n",
    "\n",
    "        # open reprojected shp layer and prepare to merge with other files\n",
    "        merged_shp = gpd.read_file(shp_layer_crs_path)\n",
    "        # merge reprojected shp file with 'pu_selected' & 'puvsp_dat' dfs                \n",
    "        # add 'id' index to enable merge with other files\n",
    "        merged_shp.insert(0, 'id', range(1, 1 + len(merged_shp)))\n",
    "        merged_shp.set_index('id')\n",
    "        # get 'pu_selected' file from 'globfile_selected' list\n",
    "        pu_selected_path = globfile_selected[0]\n",
    "        pu_selected = pd.read_csv(pu_selected_path).set_index('id')\n",
    "        # merge 'pu_selected' to shp layer (adds 'select' column, & more *)\n",
    "        merged_shp = merged_shp.merge(pu_selected, on='id')\n",
    "        # open 'puvsp.dat' from input directory\n",
    "        puvsp_path = glob(os.path.normpath(os.path.join(ecotestdir, '*', \n",
    "                                                        'input', \n",
    "                                                        'puvsp.dat')))[0]\n",
    "        # merge with shp layer to get 'amount' from puvsp\n",
    "        puvsp_dat = pd.read_csv(puvsp_path)\n",
    "        puvsp_dat = puvsp_dat.rename(columns={'pu': 'id'}).set_index('id')\n",
    "        merged_shp = merged_shp.merge(puvsp_dat, on='id')\n",
    "        \n",
    "        # use 'amount' value to calculate 'percent_of_total' \n",
    "        # (the proportion of total ecosystem extent found in each hexcell)\n",
    "        merged_shp['percent_of_total'] = (\n",
    "            merged_shp['amount']/merged_shp['US_m2'])\n",
    "#\n",
    "        # save merged shapefile as new file\n",
    "        # check if file already exists, if not create it\n",
    "        merged_shp_layer_path = os.path.normpath(os.path.join(\n",
    "            ecotest_data_path,\n",
    "            'selected_plot', \n",
    "            scen_id + \"_merged.shp\"))\n",
    "        if os.path.exists(merged_shp_layer_path):\n",
    "            print(scen_id + \"_merged.shp file check = PASS\")\n",
    "        else:\n",
    "            # save merged shp with add'l 'selected' info as new shape file\n",
    "            # THIS WOULD BE THE TIME TO CHECK FOR COLUMN NAMES >10 CHARS\n",
    "            merged_shp.to_file(merged_shp_layer_path, index=False)\n",
    "            print (scen_id + ': ' + eco + '.shp merged with ' + scen_id + \n",
    "                   \"'pu_selected' and 'puvsp.dat', saved as \" + scen_id +\n",
    "                   \"merged.shp\") \n",
    "        # verify shp file exists, and print update to screen\n",
    "        if os.path.exists(merged_shp_layer_path):\n",
    "            print(scen_id + \" : merged shapefile saved to 'selected_plot' \"\n",
    "                  \"directory\") \n",
    "        else:\n",
    "            print(scen_id + (\": Error: reprojected shapefile was not able\"\n",
    "               \" to be saved\"))\n",
    "\n",
    "        # check if reprojected tif in 'source_data' exists, if not create it\n",
    "        tif_layer_crs_path = os.path.normpath(os.path.join(\n",
    "            new_dir, 'eco_' + eco, 'source_data', \n",
    "            eco + \"_espg_\" + espg +'.tif'))\n",
    "        if os.path.exists(tif_layer_crs_path):\n",
    "            print('reprojected tif file check = PASS')\n",
    "        else:\n",
    "            # open the tif file saved at 'eco_data_path/source_data' location\n",
    "            tif_data_path = os.path.join(new_dir, 'eco_' + eco, 'source_data', \n",
    "                                         eco + '.tif')\n",
    "            tif_layer = rxr.open_rasterio(tif_data_path, \n",
    "                                          masked=True).squeeze()\n",
    "            # reproject CRS of tif - \n",
    "            # first create a rasterio crs object\n",
    "            crs_espg = CRS.from_string('EPSG:' + espg)\n",
    "            # then reproject tif using the crs object\n",
    "            tif_layer_crs = tif_layer.rio.reproject(crs_espg)\n",
    "            # create new .tif file\n",
    "            tif_layer_crs.rio.to_raster(tif_layer_crs_path)\n",
    "            # verify tif file exits, and print update to screen\n",
    "            if os.path.exists(tif_layer_crs_path):\n",
    "                print(scen_id + ': Raster reprojected to ESPG: ' + espg + \n",
    "                      \" and saved to 'source_data' directory\")\n",
    "            else:\n",
    "                print(scen_id + (\": Error: reprojected raster was not \"\n",
    "                                 \"able to be saved\"))\n",
    "\n",
    "        # get data from merged shp, to include in 'final_summary.csv'  \n",
    "        merged_shp_df = merged_shp[['id', \n",
    "                                    'amount',\n",
    "                                    'percent_of_total',\n",
    "                                    'selection',\n",
    "                                    'Short_Name',\n",
    "                                    'Current_IUCN_TH',\n",
    "                                    'current_test_level',\n",
    "                                    'KBA @ current test (m_2)',\n",
    "                                    'US_km2',\n",
    "                                    'US_m2',\n",
    "                                    '30% of US_m2',\n",
    "                                    'BLM',\n",
    "                                    'SPF',\n",
    "                                    'dir_path',]].copy()\n",
    "        \n",
    "        merged_shp_df['amount (km_2)'] = merged_shp_df['amount']/1000000   \n",
    "        \n",
    "        merged_shp_df['KBA @ current test (km_2)'] = (\n",
    "        merged_shp_df['KBA @ current test (m_2)']/1000000)     \n",
    "        \n",
    "        merged_shp_df.set_index('id')\n",
    "        \n",
    "        # Append 'merged_shp_df' to 'test_loop_ls' list, so that all \n",
    "        # information from the test loop will be incorporated into the \n",
    "        # '_second_loop_summary.csv'\n",
    "        test_loop_ls.append(merged_shp_df)\n",
    "        print('' + scen_id + '  ' + \n",
    "              \"test loop info appended to to test_loop_ls\")\n",
    "\n",
    "        ###    \n",
    "\n",
    "        # CREATE PLOT SHOWING MULITPLE LOOP'S SELECTIONS OVER THE RASTER\n",
    "        # * VISUALIZATIONS SHOWING HEXCELL SELECTION FROM BEST RUN AND \n",
    "        # HEATMAP OF HEXCELL EXTENT AS A PROPORTION OF TOTAL EXTENT\n",
    "        # solution, and save each as a .png image file\n",
    "        print ('preparing plots...')\n",
    "\n",
    "        # define raster extent for plotting\n",
    "        raster_extent = plotting_extent(tif_layer_crs,\n",
    "                                         tif_layer_crs.rio.transform())\n",
    "        \n",
    "        # get metrics to include in figtitle\n",
    "        # total amount (m2) of ecosystem included in selection\n",
    "        selected_m = merged_shp.query(\n",
    "            \"selection!='not selected'\")['amount'].sum()\n",
    "        selected_km = selected_m/1000000\n",
    "        selected_m_string = str(\"{:,.2f}\".format(selected_m))\n",
    "        selected_km_string = str(\"{:,.2f}\".format(selected_km))\n",
    "\n",
    "        # get total extent of ecosystem (from the amount column, in puvsp.dat)\n",
    "        eco_extent_km = eco_subset_df.at[eco,'US_km2']\n",
    "        eco_extent_m = eco_extent_km * 1000000\n",
    "        eco_extent_km_string = str(\"{:,.2f}\".format(eco_extent_km))\n",
    "        eco_extent_m_string = str(\"{:,.2f}\".format(eco_extent_m))\n",
    "        \n",
    "        # get Conservation Target value (currently 30% x total extent)\n",
    "        conserv_tgt_km = prop * eco_extent_km\n",
    "        conserv_tgt_km_string = str(\"{:.2f}\".format(conserv_tgt_km))\n",
    "               \n",
    "        # get selected proporion of total\n",
    "        selected_prop = selected_km / eco_extent_km\n",
    "        sel_prop_string = str(\"{:.2%}\".format(selected_prop))\n",
    "        \n",
    "        # set target2 value as string, for inclusion in figure title\n",
    "#         us_m2 = eco_subset_df.at[eco,'US_km2']*1000000\n",
    "        test_level = merged_shp_df['current_test_level'].mean()\n",
    "        test_level_string = str(\"{:.0%}\".format(test_level))\n",
    "        current_iucn_th = eco_subset_df.at[eco,'Current_IUCN_TH']\n",
    "        target2_m = (test_level * current_iucn_th * eco_extent_m).mean()\n",
    "        target2_km = target2_m/1000000\n",
    "        target2_m_string = str(\"{:,.2f}\".format(target2_m))\n",
    "        target2_km_string = str(\"{:,.2f}\".format(target2_km))\n",
    "        \n",
    "        # print figure title info to screen for validation\n",
    "        print('selected_km: ' + str(selected_km) + \n",
    "              '\\neco_extent_km: ' + str(eco_extent_km) + \n",
    "              '\\nconserv_tgt_km: ' + str(conserv_tgt_km) +\n",
    "              '\\nselected_prop: ' + str(selected_prop) +\n",
    "              '\\ntarget2_km: ' + str(target2_km) + \n",
    "              '\\ntarget2_m: ' + str(target2_m) + \n",
    "              '\\ntest_level: ' + str(test_level) +\n",
    "              '\\ncurrent_iucn_th: ' + str(current_iucn_th) +\n",
    "              '\\neco_extent_m: ' + str(eco_extent_m))\n",
    "    \n",
    "        # create strings for individual lines in figtitle\n",
    "        ft1 = (scen_id.upper() + \" - Searching for KBA @ \" + test_level_string \n",
    "               + \" Current IUCN Value\\n\")\n",
    "        ft2 = (\"Total Ecosytem Extent: \" + eco_extent_km_string + \" sq km\\n\")\n",
    "        ft3 = (\"Conservation Target: \" + conserv_tgt_km_string + \" sq km\\n\")\n",
    "        ft3 = (\"KBA target size: \" + target2_km_string + \" sq km\\n\")\n",
    "        ft4 = (\"Total Selected Ecosystem \" + selected_km_string + \"sq km\\n(\" +\n",
    "               sel_prop_string + \" of Total Extent)\")\n",
    "        \n",
    "        selected_title_txt = ft1 + ft2 + ft3 + ft4\n",
    "        \n",
    "        # PLOT (3 LAYERS) - BEST SELECTION, RASTER, AND BASEMAP\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        merged_shp.plot(column='selection',\n",
    "                        cmap='nipy_spectral_r', # orig used viridis \n",
    "                        ax=ax, \n",
    "                        alpha=0.50, \n",
    "                        legend=True)\n",
    "                          \n",
    "        ax.set(title=selected_title_txt)\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        ax.patch.set_edgecolor('black')\n",
    "        cx.add_basemap(ax=ax, crs=shp_layer_crs.crs)\n",
    "        ax.imshow(tif_layer_crs, cmap='jet', extent=raster_extent, \n",
    "                  interpolation='nearest')\n",
    "        plt.savefig((scen_id + '_pu_selections_over_raster.png'), \n",
    "                    facecolor='w', edgecolor='k', dpi=600)\n",
    "        plt.close(fig)\n",
    "        print(scen_id + \": _pu_selections_over_raster saved as .png\\n\")\n",
    "\n",
    "        # convert 'selected' plot to image and add to 'plot_im_list', so \n",
    "        # that it'll be included in final pdf of plot images\n",
    "        plot_im = glob(os.path.normpath(os.path.join(\n",
    "            os.getcwd(), scen_id + \"_pu_selections_over_raster.png\")))\n",
    "        plot_im = Image.open(plot_im[0])\n",
    "        plot_im = plot_im.convert('RGB')\n",
    "        plot_im_list.append(plot_im) \n",
    "        print(\"\\nand will be included in 'final_plots.pdf'\")\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "# loop through ecosystem info another time to create the heatmap plots (which\n",
    "# contain ecosystem level information only, this will not show any information\n",
    "# from the Marxan analysis runs)\n",
    "for eco in eco_list:\n",
    "    print('\\nbegin loop for ' + eco)\n",
    "    eco_data_path = os.path.normpath(os.path.join(new_dir, 'eco_' + eco))\n",
    "\n",
    "    os.chdir(eco_data_path)\n",
    "    \n",
    "    # define paths needed to get correct info for eco plot (no test level \n",
    "    # data needed)\n",
    "    merged_shp_layer_path = glob(os.path.normpath(os.path.join(\n",
    "        '*','selected_plot', '*_merged.shp')))[0]   \n",
    "    merged_shp = gpd.read_file(merged_shp_layer_path)\n",
    "    \n",
    "    # define raster extent for plotting\n",
    "    tif_layer_crs_path = os.path.normpath(os.path.join(\n",
    "            os.getcwd(), 'source_data', eco + \"_espg_\" + espg +'.tif'))\n",
    "\n",
    "    tif_layer_crs = rxr.open_rasterio(tif_layer_crs_path, \n",
    "                                      masked=True).squeeze()\n",
    "    raster_extent = plotting_extent(tif_layer_crs,\n",
    "                                    tif_layer_crs.rio.transform())\n",
    "    \n",
    "    # PLOT (3 LAYERS) - 'PERCENT_OF_TOTAL', RASTER, AND BASEMAP\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "    fig.suptitle(t=(\"Percentage of the Total Ecosystem Extent \"\n",
    "                 \"Containined in Each Hexcell\\n\"))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    merged_shp.plot(column='percent_of', \n",
    "                    cmap='RdYlGn', \n",
    "                    ax=ax, \n",
    "                    alpha=0.65, \n",
    "                    legend=True)\n",
    "#                     legend_kwds={'loc': 'middle left'})\n",
    "    ax.set(title=(eco + \": Percentage of Total Ecosystem Extent \"\n",
    "                  \"Containined in Each Hexcell\\n\"))\n",
    "    cx.add_basemap(ax=ax, crs=merged_shp.crs, \n",
    "                    source=cx.providers.CartoDB.Positron) \n",
    "    ax.imshow(tif_layer_crs, cmap='jet', extent=raster_extent,\n",
    "      interpolation='nearest')\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    ax.set_title(eco.upper())\n",
    "    plt.savefig((eco + '_hexcell_as%_total_extent.png'), facecolor='w', \n",
    "                edgecolor='k', dpi=600)\n",
    "    print(eco + \"_hexcell_as%_total_extent saved as .png'\\n\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # convert '% of total' plot to image and add to 'plot_im_list', so \n",
    "    # that it'll be included in final pdf of plot images\n",
    "    plot_im = glob(os.path.normpath(os.path.join(\n",
    "    os.getcwd(), eco + \"_hexcell_as%_total_extent.png\")))\n",
    "    plot_im = Image.open(plot_im[0])\n",
    "    plot_im = plot_im.convert('RGB')\n",
    "    plot_im_list.append(plot_im) \n",
    "    print(\"\\nand will be included in 'final_plots.pdf'\")\n",
    "\n",
    "# combine all dfs stored in the 'test_loop_ls' list into one pandas dataframe,\n",
    "# then save that dataframe as '_shp_summary.csv'\n",
    "final_shp_summary_df = pd.concat(test_loop_ls)\n",
    "final_shp_summary_df.to_csv(os.path.normpath(os.path.join(\n",
    "    new_dir, testrun_basename + '_shp_summary.csv')), index=False)\n",
    "print(\"\\n\" + testrun_basename + \"'_shp_summary.csv' saved to \" + new_dir)\n",
    "\n",
    "# save plot images to pdf (*CURRENTLY DUPLICATES THE 1ST IMAGE, NEEDS FIX)\n",
    "plots_pdf_path = os.path.normpath(os.path.join(new_dir, \n",
    "                                               'combined_plots.pdf'))\n",
    "plot_im_list[0].save(plots_pdf_path, save_all=True, \n",
    "                     append_images=plot_im_list)\n",
    "\n",
    "print(\"\\n'final_plots.pdf' saved to \" + new_dir)\n",
    "\n",
    "print('\\nloop completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c38af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 424.844,
   "position": {
    "height": "717px",
    "left": "2236px",
    "right": "20px",
    "top": "176px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
